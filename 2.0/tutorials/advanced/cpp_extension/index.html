
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://pytorch.apachecn.org/2.0/tutorials/advanced/cpp_extension/">
      
      
        <link rel="prev" href="../../intermediate/custom_function_conv_bn_tutorial/">
      
      
        <link rel="next" href="../torch_script_custom_ops/">
      
      
      <link rel="icon" href="../../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.3">
    
    
      
        <title>Custom C++ and CUDA Extensions - 【布客】PyTorch 中文翻译</title>
      
    
    
      <link rel="stylesheet" href="../../../../assets/stylesheets/main.d7758b05.min.css">
      
      
  
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
  
  <style>:root{--md-admonition-icon--note:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M1 7.775V2.75C1 1.784 1.784 1 2.75 1h5.025c.464 0 .91.184 1.238.513l6.25 6.25a1.75 1.75 0 0 1 0 2.474l-5.026 5.026a1.75 1.75 0 0 1-2.474 0l-6.25-6.25A1.75 1.75 0 0 1 1 7.775m1.5 0c0 .066.026.13.073.177l6.25 6.25a.25.25 0 0 0 .354 0l5.025-5.025a.25.25 0 0 0 0-.354l-6.25-6.25a.25.25 0 0 0-.177-.073H2.75a.25.25 0 0 0-.25.25ZM6 5a1 1 0 1 1 0 2 1 1 0 0 1 0-2"/></svg>');--md-admonition-icon--abstract:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M2.5 1.75v11.5c0 .138.112.25.25.25h3.17a.75.75 0 0 1 0 1.5H2.75A1.75 1.75 0 0 1 1 13.25V1.75C1 .784 1.784 0 2.75 0h8.5C12.216 0 13 .784 13 1.75v7.736a.75.75 0 0 1-1.5 0V1.75a.25.25 0 0 0-.25-.25h-8.5a.25.25 0 0 0-.25.25m13.274 9.537zl-4.557 4.45a.75.75 0 0 1-1.055-.008l-1.943-1.95a.75.75 0 0 1 1.062-1.058l1.419 1.425 4.026-3.932a.75.75 0 1 1 1.048 1.074M4.75 4h4.5a.75.75 0 0 1 0 1.5h-4.5a.75.75 0 0 1 0-1.5M4 7.75A.75.75 0 0 1 4.75 7h2a.75.75 0 0 1 0 1.5h-2A.75.75 0 0 1 4 7.75"/></svg>');--md-admonition-icon--info:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8m8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13M6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75M8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2"/></svg>');--md-admonition-icon--tip:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M3.499.75a.75.75 0 0 1 1.5 0v.996C5.9 2.903 6.793 3.65 7.662 4.376l.24.202c-.036-.694.055-1.422.426-2.163C9.1.873 10.794-.045 12.622.26 14.408.558 16 1.94 16 4.25c0 1.278-.954 2.575-2.44 2.734l.146.508.065.22c.203.701.412 1.455.476 2.226.142 1.707-.4 3.03-1.487 3.898C11.714 14.671 10.27 15 8.75 15h-6a.75.75 0 0 1 0-1.5h1.376a4.5 4.5 0 0 1-.563-1.191 3.84 3.84 0 0 1-.05-2.063 4.65 4.65 0 0 1-2.025-.293.75.75 0 0 1 .525-1.406c1.357.507 2.376-.006 2.698-.318l.009-.01a.747.747 0 0 1 1.06 0 .75.75 0 0 1-.012 1.074c-.912.92-.992 1.835-.768 2.586.221.74.745 1.337 1.196 1.621H8.75c1.343 0 2.398-.296 3.074-.836.635-.507 1.036-1.31.928-2.602-.05-.603-.216-1.224-.422-1.93l-.064-.221c-.12-.407-.246-.84-.353-1.29a2.4 2.4 0 0 1-.507-.441 3.1 3.1 0 0 1-.633-1.248.75.75 0 0 1 1.455-.364c.046.185.144.436.31.627.146.168.353.305.712.305.738 0 1.25-.615 1.25-1.25 0-1.47-.95-2.315-2.123-2.51-1.172-.196-2.227.387-2.706 1.345-.46.92-.27 1.774.019 3.062l.042.19.01.05c.348.443.666.949.94 1.553a.75.75 0 1 1-1.365.62c-.553-1.217-1.32-1.94-2.3-2.768L6.7 5.527c-.814-.68-1.75-1.462-2.692-2.619a3.7 3.7 0 0 0-1.023.88c-.406.495-.663 1.036-.722 1.508.116.122.306.21.591.239.388.038.797-.06 1.032-.19a.75.75 0 0 1 .728 1.31c-.515.287-1.23.439-1.906.373-.682-.067-1.473-.38-1.879-1.193L.75 5.677V5.5c0-.984.48-1.94 1.077-2.664.46-.559 1.05-1.055 1.673-1.353z"/></svg>');--md-admonition-icon--success:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.75.75 0 0 1 .018-1.042.75.75 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0"/></svg>');--md-admonition-icon--question:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8m8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13M6.92 6.085h.001a.749.749 0 1 1-1.342-.67c.169-.339.436-.701.849-.977C6.845 4.16 7.369 4 8 4a2.76 2.76 0 0 1 1.637.525c.503.377.863.965.863 1.725 0 .448-.115.83-.329 1.15-.205.307-.47.513-.692.662-.109.072-.22.138-.313.195l-.006.004a6 6 0 0 0-.26.16 1 1 0 0 0-.276.245.75.75 0 0 1-1.248-.832c.184-.264.42-.489.692-.661q.154-.1.313-.195l.007-.004c.1-.061.182-.11.258-.161a1 1 0 0 0 .277-.245C8.96 6.514 9 6.427 9 6.25a.61.61 0 0 0-.262-.525A1.27 1.27 0 0 0 8 5.5c-.369 0-.595.09-.74.187a1 1 0 0 0-.34.398M9 11a1 1 0 1 1-2 0 1 1 0 0 1 2 0"/></svg>');--md-admonition-icon--warning:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M6.457 1.047c.659-1.234 2.427-1.234 3.086 0l6.082 11.378A1.75 1.75 0 0 1 14.082 15H1.918a1.75 1.75 0 0 1-1.543-2.575Zm1.763.707a.25.25 0 0 0-.44 0L1.698 13.132a.25.25 0 0 0 .22.368h12.164a.25.25 0 0 0 .22-.368Zm.53 3.996v2.5a.75.75 0 0 1-1.5 0v-2.5a.75.75 0 0 1 1.5 0M9 11a1 1 0 1 1-2 0 1 1 0 0 1 2 0"/></svg>');--md-admonition-icon--failure:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M2.344 2.343za8 8 0 0 1 11.314 11.314A8.002 8.002 0 0 1 .234 10.089a8 8 0 0 1 2.11-7.746m1.06 10.253a6.5 6.5 0 1 0 9.108-9.275 6.5 6.5 0 0 0-9.108 9.275M6.03 4.97 8 6.94l1.97-1.97a.749.749 0 0 1 1.275.326.75.75 0 0 1-.215.734L9.06 8l1.97 1.97a.749.749 0 0 1-.326 1.275.75.75 0 0 1-.734-.215L8 9.06l-1.97 1.97a.749.749 0 0 1-1.275-.326.75.75 0 0 1 .215-.734L6.94 8 4.97 6.03a.75.75 0 0 1 .018-1.042.75.75 0 0 1 1.042-.018"/></svg>');--md-admonition-icon--danger:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M9.504.43a1.516 1.516 0 0 1 2.437 1.713L10.415 5.5h2.123c1.57 0 2.346 1.909 1.22 3.004l-7.34 7.142a1.25 1.25 0 0 1-.871.354h-.302a1.25 1.25 0 0 1-1.157-1.723L5.633 10.5H3.462c-1.57 0-2.346-1.909-1.22-3.004zm1.047 1.074L3.286 8.571A.25.25 0 0 0 3.462 9H6.75a.75.75 0 0 1 .694 1.034l-1.713 4.188 6.982-6.793A.25.25 0 0 0 12.538 7H9.25a.75.75 0 0 1-.683-1.06l2.008-4.418.003-.006-.004-.009-.006-.006-.008-.001q-.005 0-.009.004"/></svg>');--md-admonition-icon--bug:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M4.72.22a.75.75 0 0 1 1.06 0l1 .999a3.5 3.5 0 0 1 2.441 0l.999-1a.748.748 0 0 1 1.265.332.75.75 0 0 1-.205.729l-.775.776c.616.63.995 1.493.995 2.444v.327q0 .15-.025.292c.408.14.764.392 1.029.722l1.968-.787a.75.75 0 0 1 .556 1.392L13 7.258V9h2.25a.75.75 0 0 1 0 1.5H13v.5q-.002.615-.141 1.186l2.17.868a.75.75 0 0 1-.557 1.392l-2.184-.873A5 5 0 0 1 8 16a5 5 0 0 1-4.288-2.427l-2.183.873a.75.75 0 0 1-.558-1.392l2.17-.868A5 5 0 0 1 3 11v-.5H.75a.75.75 0 0 1 0-1.5H3V7.258L.971 6.446a.75.75 0 0 1 .558-1.392l1.967.787c.265-.33.62-.583 1.03-.722a1.7 1.7 0 0 1-.026-.292V4.5c0-.951.38-1.814.995-2.444L4.72 1.28a.75.75 0 0 1 0-1.06m.53 6.28a.75.75 0 0 0-.75.75V11a3.5 3.5 0 1 0 7 0V7.25a.75.75 0 0 0-.75-.75ZM6.173 5h3.654A.17.17 0 0 0 10 4.827V4.5a2 2 0 1 0-4 0v.327c0 .096.077.173.173.173"/></svg>');--md-admonition-icon--example:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M5 5.782V2.5h-.25a.75.75 0 0 1 0-1.5h6.5a.75.75 0 0 1 0 1.5H11v3.282l3.666 5.76C15.619 13.04 14.543 15 12.767 15H3.233c-1.776 0-2.852-1.96-1.899-3.458Zm-2.4 6.565a.75.75 0 0 0 .633 1.153h9.534a.75.75 0 0 0 .633-1.153L12.225 10.5h-8.45ZM9.5 2.5h-3V6c0 .143-.04.283-.117.403L4.73 9h6.54L9.617 6.403A.75.75 0 0 1 9.5 6Z"/></svg>');--md-admonition-icon--quote:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path d="M1.75 2.5h10.5a.75.75 0 0 1 0 1.5H1.75a.75.75 0 0 1 0-1.5m4 5h8.5a.75.75 0 0 1 0 1.5h-8.5a.75.75 0 0 1 0-1.5m0 5h8.5a.75.75 0 0 1 0 1.5h-8.5a.75.75 0 0 1 0-1.5M2.5 7.75v6a.75.75 0 0 1-1.5 0v-6a.75.75 0 0 1 1.5 0"/></svg>');}</style>



    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#c-cuda" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../../.." title="【布客】PyTorch 中文翻译" class="md-header__button md-logo" aria-label="【布客】PyTorch 中文翻译" data-md-component="logo">
      
  <img src="https://data.dafeiyang.cn/images/logo/logo_green.webp" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            【布客】PyTorch 中文翻译
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Custom C++ and CUDA Extensions
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/apachecn/pytorch-doc-zh" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    apachecn/pytorch-doc-zh
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../.." title="【布客】PyTorch 中文翻译" class="md-nav__button md-logo" aria-label="【布客】PyTorch 中文翻译" data-md-component="logo">
      
  <img src="https://data.dafeiyang.cn/images/logo/logo_green.webp" alt="logo">

    </a>
    【布客】PyTorch 中文翻译
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/apachecn/pytorch-doc-zh" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    apachecn/pytorch-doc-zh
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PyTorch 中文文档 & 教程
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    PyTorch 新特性
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            PyTorch 新特性
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../LatestChanges/PyTorch_V2.6/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    V2.6
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../LatestChanges/PyTorch_V2.5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    V2.5
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../LatestChanges/PyTorch_V2.4/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    V2.4
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../LatestChanges/PyTorch_V2.3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    V2.3
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../LatestChanges/PyTorch_V2.2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    V2.2
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../LatestChanges/PyTorch_V2.1/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    V2.1
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../LatestChanges/PyTorch_V2.0/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    V2.0
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../LatestChanges/PyTorch_V1.13/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    V1.13
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../LatestChanges/PyTorch_V1.12/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    V1.12
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../LatestChanges/PyTorch_V1.11/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    V1.11
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../LatestChanges/PyTorch_V1.10/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    V1.10
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../LatestChanges/PyTorch_V1.9/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    V1.9
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../LatestChanges/PyTorch_V1.8/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    V1.8
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../LatestChanges/PyTorch_V1.7/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    V1.7
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../LatestChanges/PyTorch_V1.6/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    V1.6
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../LatestChanges/PyTorch_V1.5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    V1.5
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../LatestChanges/PyTorch_V1.4/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    V1.4
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../LatestChanges/PyTorch_V1.3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    V1.3
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../LatestChanges/PyTorch_V1.2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    V1.2
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    PyTorch 2.x 中文文档 & 教程
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            PyTorch 2.x 中文文档 & 教程
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1" checked>
        
          
          <label class="md-nav__link" for="__nav_3_1" id="__nav_3_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    中文教程
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_1_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3_1">
            <span class="md-nav__icon md-icon"></span>
            中文教程
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_1" >
        
          
          <label class="md-nav__link" for="__nav_3_1_1" id="__nav_3_1_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    PyTorch Recipes
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_1">
            <span class="md-nav__icon md-icon"></span>
            PyTorch Recipes
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../recipes/recipes_index/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    See All Recipes
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../prototype/prototype_index/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    See All Prototype Recipes
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_2" >
        
          
          <label class="md-nav__link" for="__nav_3_1_2" id="__nav_3_1_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Introduction to PyTorch
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_2">
            <span class="md-nav__icon md-icon"></span>
            Introduction to PyTorch
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../beginner/basics/intro/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Learn the Basics
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../beginner/basics/quickstart_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Quickstart
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../beginner/basics/tensorqs_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Tensors
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../beginner/basics/data_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Datasets & DataLoaders
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../beginner/basics/transforms_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Transforms
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../beginner/basics/buildmodel_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Build the Neural Network
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../beginner/basics/autogradqs_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Automatic Differentiation with torch.autograd
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../beginner/basics/optimization_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Optimizing Model Parameters
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../beginner/basics/saveloadrun_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Save and Load the Model
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_3" >
        
          
          <label class="md-nav__link" for="__nav_3_1_3" id="__nav_3_1_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Introduction to PyTorch on YouTube
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_3">
            <span class="md-nav__icon md-icon"></span>
            Introduction to PyTorch on YouTube
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../beginner/introyt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Introduction to PyTorch - YouTube Series
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../beginner/introyt/introyt1_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Introduction to PyTorch
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../beginner/introyt/tensors_deeper_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Introduction to PyTorch Tensors
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../beginner/introyt/autogradyt_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    The Fundamentals of Autograd
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../beginner/introyt/modelsyt_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Building Models with PyTorch
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../beginner/introyt/tensorboardyt_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PyTorch TensorBoard Support
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../beginner/introyt/trainingyt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Training with PyTorch
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../beginner/introyt/captumyt/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Model Understanding with Captum
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_4" >
        
          
          <label class="md-nav__link" for="__nav_3_1_4" id="__nav_3_1_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Learning PyTorch
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_4">
            <span class="md-nav__icon md-icon"></span>
            Learning PyTorch
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../beginner/deep_learning_60min_blitz/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Deep Learning with PyTorch: A 60 Minute Blitz
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../beginner/pytorch_with_examples/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Learning PyTorch with Examples
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../beginner/nn_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    What is torch.nn really?
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../intermediate/tensorboard_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Visualizing Models, Data, and Training with TensorBoard
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_5" >
        
          
          <label class="md-nav__link" for="__nav_3_1_5" id="__nav_3_1_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Image and Video
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_5">
            <span class="md-nav__icon md-icon"></span>
            Image and Video
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../intermediate/torchvision_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    TorchVision Object Detection Finetuning Tutorial
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../beginner/transfer_learning_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Transfer Learning for Computer Vision Tutorial
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../beginner/fgsm_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Adversarial Example Generation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../beginner/dcgan_faces_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    DCGAN Tutorial
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../intermediate/spatial_transformer_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Spatial Transformer Networks Tutorial
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../beginner/vt_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Optimizing Vision Transformer Model for Deployment
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../intermediate/tiatoolbox_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Whole Slide Image Classification Using PyTorch and TIAToolbox
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_6" >
        
          
          <label class="md-nav__link" for="__nav_3_1_6" id="__nav_3_1_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Audio
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_6">
            <span class="md-nav__icon md-icon"></span>
            Audio
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../beginner/audio_io_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Audio I/O
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../beginner/audio_resampling_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Audio Resampling
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../beginner/audio_data_augmentation_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Audio Data Augmentation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../beginner/audio_feature_extractions_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Audio Feature Extractions
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../beginner/audio_feature_augmentation_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Audio Feature Augmentation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../beginner/audio_datasets_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Audio Datasets
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../intermediate/speech_recognition_pipeline_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Speech Recognition with Wav2Vec2
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../intermediate/text_to_speech_with_torchaudio/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Text-to-speech with Tacotron2
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../intermediate/forced_alignment_with_torchaudio_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Forced Alignment with Wav2Vec2
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_7" >
        
          
          <label class="md-nav__link" for="__nav_3_1_7" id="__nav_3_1_7_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Text
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_7">
            <span class="md-nav__icon md-icon"></span>
            Text
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../beginner/bettertransformer_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Fast Transformer Inference with Better Transformer
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../intermediate/char_rnn_classification_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    NLP From Scratch: Classifying Names with a Character-Level RNN
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../intermediate/char_rnn_generation_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    NLP From Scratch: Generating Names with a Character-Level RNN
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../intermediate/seq2seq_translation_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    NLP From Scratch: Translation with a Sequence to Sequence Network and Attention
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../beginner/text_sentiment_ngrams_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Text classification with the torchtext library
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../beginner/translation_transformer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Language Translation with nn.Transformer and torchtext
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../beginner/torchtext_custom_dataset_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Preprocess custom text dataset using Torchtext
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_8" >
        
          
          <label class="md-nav__link" for="__nav_3_1_8" id="__nav_3_1_8_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Backends
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_8">
            <span class="md-nav__icon md-icon"></span>
            Backends
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../beginner/onnx/intro_onnx/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Introduction to ONNX
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_9" >
        
          
          <label class="md-nav__link" for="__nav_3_1_9" id="__nav_3_1_9_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Reinforcement Learning
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_9_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_9">
            <span class="md-nav__icon md-icon"></span>
            Reinforcement Learning
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../intermediate/reinforcement_q_learning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Reinforcement Learning (DQN) Tutorial
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../intermediate/reinforcement_ppo/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Reinforcement Learning (PPO) with TorchRL Tutorial
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../intermediate/mario_rl_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Train a Mario-playing RL Agent
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pendulum/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Pendulum: Writing your environment and transforms with TorchRL
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_10" >
        
          
          <label class="md-nav__link" for="__nav_3_1_10" id="__nav_3_1_10_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Deploying PyTorch Models in Production
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_10_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_10">
            <span class="md-nav__icon md-icon"></span>
            Deploying PyTorch Models in Production
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../beginner/onnx/intro_onnx/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Introduction to ONNX
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../intermediate/flask_rest_api_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Deploying PyTorch in Python via a REST API with Flask
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../beginner/Intro_to_TorchScript_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Introduction to TorchScript
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../cpp_export/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Loading a TorchScript Model in C++
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../super_resolution_with_onnxruntime/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    (optional) Exporting a Model from PyTorch to ONNX and Running it using ONNX Runtime
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../intermediate/realtime_rpi/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Real Time Inference on Raspberry Pi 4 (30 fps!)
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_11" >
        
          
          <label class="md-nav__link" for="__nav_3_1_11" id="__nav_3_1_11_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Profiling PyTorch
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_11_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_11">
            <span class="md-nav__icon md-icon"></span>
            Profiling PyTorch
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../beginner/profiler/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Profiling your PyTorch Module
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../beginner/hta_intro_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Introduction to Holistic Trace Analysis
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../beginner/hta_trace_diff_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Trace Diff using Holistic Trace Analysis
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_12" >
        
          
          <label class="md-nav__link" for="__nav_3_1_12" id="__nav_3_1_12_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Code Transforms with FX
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_12_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_12">
            <span class="md-nav__icon md-icon"></span>
            Code Transforms with FX
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../intermediate/fx_conv_bn_fuser/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    (beta) Building a Convolution/Batch Norm fuser in FX
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../intermediate/fx_profiling_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    (beta) Building a Simple CPU Performance Profiler with FX
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_13" >
        
          
          <label class="md-nav__link" for="__nav_3_1_13" id="__nav_3_1_13_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Frontend APIs
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_13_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_13">
            <span class="md-nav__icon md-icon"></span>
            Frontend APIs
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../intermediate/memory_format_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    (beta) Channels Last Memory Format in PyTorch
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../intermediate/forward_ad_usage/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Forward-mode Automatic Differentiation (Beta)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../intermediate/jacobians_hessians/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Jacobians, Hessians, hvp, vhp, and more: composing function transforms
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../intermediate/ensembling/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Model ensembling
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../intermediate/per_sample_grads/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Per-sample-gradients
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../cpp_frontend/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Using the PyTorch C++ Frontend
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../torch-script-parallelism/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Dynamic Parallelism in TorchScript
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../cpp_autograd/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Autograd in C++ Frontend
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_14" checked>
        
          
          <label class="md-nav__link" for="__nav_3_1_14" id="__nav_3_1_14_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Extending PyTorch
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_14_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3_1_14">
            <span class="md-nav__icon md-icon"></span>
            Extending PyTorch
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../intermediate/custom_function_double_backward_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Double Backward with Custom Functions
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../intermediate/custom_function_conv_bn_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Fusing Convolution and Batch Norm using Custom Function
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Custom C++ and CUDA Extensions
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Custom C++ and CUDA Extensions
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    <span class="md-ellipsis">
      动机和示例 ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#c" class="md-nav__link">
    <span class="md-ellipsis">
      编写 C++ 扩展 ¶
    </span>
  </a>
  
    <nav class="md-nav" aria-label="编写 C++ 扩展 ¶">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    <span class="md-ellipsis">
      使用
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#c-op" class="md-nav__link">
    <span class="md-ellipsis">
      编写 C++ Op ¶
    </span>
  </a>
  
    <nav class="md-nav" aria-label="编写 C++ Op ¶">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    <span class="md-ellipsis">
      正向传递 ¶
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    <span class="md-ellipsis">
      向后传递 ¶
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#python" class="md-nav__link">
    <span class="md-ellipsis">
      绑定到 Python ¶
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_5" class="md-nav__link">
    <span class="md-ellipsis">
      使用您的扩展 ¶
    </span>
  </a>
  
    <nav class="md-nav" aria-label="使用您的扩展 ¶">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_6" class="md-nav__link">
    <span class="md-ellipsis">
      性能比较 ¶
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpu" class="md-nav__link">
    <span class="md-ellipsis">
      GPU 设备上的性能 ¶
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#jit" class="md-nav__link">
    <span class="md-ellipsis">
      JIT 编译扩展 ¶
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ccuda" class="md-nav__link">
    <span class="md-ellipsis">
      编写混合 C++/CUDA 扩展 ¶
    </span>
  </a>
  
    <nav class="md-nav" aria-label="编写混合 C++/CUDA 扩展 ¶">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_7" class="md-nav__link">
    <span class="md-ellipsis">
      使用访问器 ¶
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ccuda-pytorch" class="md-nav__link">
    <span class="md-ellipsis">
      将 C++/CUDA 操作与 PyTorch 集成 ¶
    </span>
  </a>
  
    <nav class="md-nav" aria-label="将 C++/CUDA 操作与 PyTorch 集成 ¶">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_8" class="md-nav__link">
    <span class="md-ellipsis">
      性能比较 ¶
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_9" class="md-nav__link">
    <span class="md-ellipsis">
      结论 ¶
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../torch_script_custom_ops/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Extending TorchScript with Custom C++ Operators
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../torch_script_custom_classes/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Extending TorchScript with Custom C++ Classes
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../dispatcher/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Registering a Dispatched Operator in C++
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../extend_dispatcher/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Extending dispatcher for a new backend in C++
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../privateuseone/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Facilitating New Backend Integration by PrivateUse1
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_15" >
        
          
          <label class="md-nav__link" for="__nav_3_1_15" id="__nav_3_1_15_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Model Optimization
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_15_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_15">
            <span class="md-nav__icon md-icon"></span>
            Model Optimization
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../beginner/profiler/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Profiling your PyTorch Module
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../intermediate/tensorboard_profiler_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PyTorch Profiler With TensorBoard
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../beginner/hyperparameter_tuning_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Hyperparameter tuning with Ray Tune
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../beginner/vt_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Optimizing Vision Transformer Model for Deployment
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../intermediate/parametrizations/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Parametrizations Tutorial
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../intermediate/pruning_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Pruning Tutorial
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../dynamic_quantization_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    (beta) Dynamic Quantization on an LSTM Word Language Model
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../intermediate/dynamic_quantization_bert_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    (beta) Dynamic Quantization on BERT
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../intermediate/quantized_transfer_learning_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    (beta) Quantized Transfer Learning for Computer Vision Tutorial
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../static_quantization_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    (beta) Static Quantization with Eager Mode in PyTorch
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../intermediate/torchserve_with_ipex/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Grokking PyTorch Intel CPU performance from first principles
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../intermediate/torchserve_with_ipex_2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Grokking PyTorch Intel CPU performance from first principles (Part 2)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../intermediate/nvfuser_intro_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Getting Started - Accelerate Your Scripts with nvFuser
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../intermediate/ax_multiobjective_nas_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Multi-Objective NAS with Ax
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../intermediate/torch_compile_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Introduction to torch.compile
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../intermediate/inductor_debug_cpu/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Inductor CPU backend debugging and profiling
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../intermediate/scaled_dot_product_attention_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    (Beta) Implementing High-Performance Transformers with Scaled Dot Product Attention (SDPA)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../intermediate/scaled_dot_product_attention_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    (Beta) Implementing High-Performance Transformers with Scaled Dot Product Attention (SDPA)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../intermediate/scaled_dot_product_attention_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    (Beta) Implementing High-Performance Transformers with Scaled Dot Product Attention (SDPA)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../intermediate/scaled_dot_product_attention_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    (Beta) Implementing High-Performance Transformers with Scaled Dot Product Attention (SDPA)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../beginner/knowledge_distillation_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Knowledge Distillation Tutorial
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_16" >
        
          
          <label class="md-nav__link" for="__nav_3_1_16" id="__nav_3_1_16_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Parallel and Distributed Training
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_16_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_16">
            <span class="md-nav__icon md-icon"></span>
            Parallel and Distributed Training
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../distributed/home/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Distributed and Parallel Training Tutorials
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../beginner/dist_overview/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PyTorch Distributed Overview
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../beginner/ddp_series_intro/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Distributed Data Parallel in PyTorch - Video Tutorials
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../intermediate/model_parallel_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Single-Machine Model Parallel Best Practices
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../intermediate/ddp_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Getting Started with Distributed Data Parallel
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../intermediate/dist_tuto/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Writing Distributed Applications with PyTorch
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../intermediate/FSDP_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Getting Started with Fully Sharded Data Parallel(FSDP)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../intermediate/FSDP_adavnced_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Advanced Model Training with Fully Sharded Data Parallel (FSDP)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../intermediate/TP_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Large Scale Transformer model training with Tensor Parallel (TP)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../intermediate/process_group_cpp_extension_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Customize Process Group Backends Using Cpp Extensions
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../intermediate/rpc_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Getting Started with Distributed RPC Framework
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../intermediate/rpc_param_server_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Implementing a Parameter Server Using Distributed RPC Framework
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../intermediate/dist_pipeline_parallel_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Distributed Pipeline Parallelism Using RPC
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../intermediate/rpc_async_execution/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Implementing Batch RPC Processing Using Asynchronous Executions
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../rpc_ddp_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Combining Distributed DataParallel with Distributed RPC Framework
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ddp_pipeline/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Training Transformer models using Distributed Data Parallel and Pipeline Parallelism
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generic_join/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Distributed Training with Uneven Inputs Using the Join Context Manager
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_17" >
        
          
          <label class="md-nav__link" for="__nav_3_1_17" id="__nav_3_1_17_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Edge with ExecuTorch
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_17_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_17">
            <span class="md-nav__icon md-icon"></span>
            Edge with ExecuTorch
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="https://pytorch.org/executorch/stable/tutorials/export-to-executorch-tutorial.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Exporting to ExecuTorch Tutorial
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="https://pytorch.org/executorch/stable/running-a-model-cpp-tutorial.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Running an ExecuTorch Model in C++ Tutorial
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="https://pytorch.org/executorch/stable/tutorials/sdk-integration-tutorial.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Using the ExecuTorch SDK to Profile a Model
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="https://pytorch.org/executorch/stable/demo-apps-ios.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Building an ExecuTorch iOS Demo App
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="https://pytorch.org/executorch/stable/demo-apps-android.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Building an ExecuTorch Android Demo App
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="https://pytorch.org/executorch/stable/examples-end-to-end-to-lower-model-to-delegate.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Lowering a Model as a Delegate
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_18" >
        
          
          <label class="md-nav__link" for="__nav_3_1_18" id="__nav_3_1_18_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Recommendation Systems
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_18_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_18">
            <span class="md-nav__icon md-icon"></span>
            Recommendation Systems
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../intermediate/torchrec_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Introduction to TorchRec
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../sharding/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Exploring TorchRec sharding
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_19" >
        
          
          <label class="md-nav__link" for="__nav_3_1_19" id="__nav_3_1_19_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Multimodality
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_19_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_19">
            <span class="md-nav__icon md-icon"></span>
            Multimodality
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../beginner/flava_finetuning_tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    TorchMultimodal Tutorial: Finetuning FLAVA
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2" >
        
          
          <label class="md-nav__link" for="__nav_3_2" id="__nav_3_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    中文文档
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_2">
            <span class="md-nav__icon md-icon"></span>
            中文文档
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../docs/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    介绍
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="https://pytorch.org/docs/stable/index.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Pytorch
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="https://pytorch.org/audio/stable/index.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Torchaudio
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="https://pytorch.org/text/stable/index.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    TorchText
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="https://pytorch.org/vision/stable/index.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    TorchVision
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="https://pytorch.org/torcharrow/beta/index.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    TorchArrow
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="https://pytorch.org/torchrec/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    TorchRec
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="https://pytorch.org/serve/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    TorchServe
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="https://pytorch.org/torchx/latest/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    TorchX
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="https://pytorch.org/xla/release/2.3/index.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PyTorch on XLA Devices
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="https://pytorch1x.apachecn.org" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PyTorch 1.7 中文文档
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="https://pytorch1x.apachecn.org" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PyTorch 1.4 中文文档 & 教程
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="https://pytorch1x.apachecn.org" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PyTorch 1.0 中文文档 & 教程
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="https://pytorch0x.apachecn.org" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PyTorch 0.4 中文文档
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="https://pytorch0x.apachecn.org" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PyTorch 0.3 中文文档 & 教程
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="https://pytorch0x.apachecn.org" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    PyTorch 0.2 中文文档
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../contrib/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    贡献指南
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="https://www.apachecn.org/about" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    关于我们
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="https://www.apachecn.org/join" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    加入我们
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="https://docs.apachecn.org" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    中文资源合集
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    <span class="md-ellipsis">
      动机和示例 ¶
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#c" class="md-nav__link">
    <span class="md-ellipsis">
      编写 C++ 扩展 ¶
    </span>
  </a>
  
    <nav class="md-nav" aria-label="编写 C++ 扩展 ¶">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    <span class="md-ellipsis">
      使用
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#c-op" class="md-nav__link">
    <span class="md-ellipsis">
      编写 C++ Op ¶
    </span>
  </a>
  
    <nav class="md-nav" aria-label="编写 C++ Op ¶">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    <span class="md-ellipsis">
      正向传递 ¶
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    <span class="md-ellipsis">
      向后传递 ¶
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#python" class="md-nav__link">
    <span class="md-ellipsis">
      绑定到 Python ¶
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_5" class="md-nav__link">
    <span class="md-ellipsis">
      使用您的扩展 ¶
    </span>
  </a>
  
    <nav class="md-nav" aria-label="使用您的扩展 ¶">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_6" class="md-nav__link">
    <span class="md-ellipsis">
      性能比较 ¶
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpu" class="md-nav__link">
    <span class="md-ellipsis">
      GPU 设备上的性能 ¶
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#jit" class="md-nav__link">
    <span class="md-ellipsis">
      JIT 编译扩展 ¶
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ccuda" class="md-nav__link">
    <span class="md-ellipsis">
      编写混合 C++/CUDA 扩展 ¶
    </span>
  </a>
  
    <nav class="md-nav" aria-label="编写混合 C++/CUDA 扩展 ¶">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_7" class="md-nav__link">
    <span class="md-ellipsis">
      使用访问器 ¶
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ccuda-pytorch" class="md-nav__link">
    <span class="md-ellipsis">
      将 C++/CUDA 操作与 PyTorch 集成 ¶
    </span>
  </a>
  
    <nav class="md-nav" aria-label="将 C++/CUDA 操作与 PyTorch 集成 ¶">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_8" class="md-nav__link">
    <span class="md-ellipsis">
      性能比较 ¶
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_9" class="md-nav__link">
    <span class="md-ellipsis">
      结论 ¶
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
    <a href="https://github.com/apachecn/pytorch-doc-zh/edit/master/docs/2.0/tutorials/advanced/cpp_extension.md" title="Edit this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/apachecn/pytorch-doc-zh/raw/master/docs/2.0/tutorials/advanced/cpp_extension.md" title="View source of this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"/></svg>
    </a>
  


<h1 id="c-cuda">自定义 C++ 和 CUDA 扩展 <a href="#custom-c-and-cuda-extensions" title="永久链接到此标题">¶</a></h1>
<blockquote>
<p>译者：<a href="https://github.com/jiangzhonglian">片刻小哥哥</a></p>
<p>项目地址：<a href="https://pytorch.apachecn.org/2.0/tutorials/advanced/cpp_extension">https://pytorch.apachecn.org/2.0/tutorials/advanced/cpp_extension</a></p>
<p>原始地址：<a href="https://pytorch.org/tutorials/advanced/cpp_extension.html">https://pytorch.org/tutorials/advanced/cpp_extension.html</a></p>
</blockquote>
<p><strong>作者</strong> 
 :
 <a href="https://www.goldsborough.me/">Peter Goldsborough</a></p>
<p>PyTorch 提供了大量与神经网络、任意tensor代数、数据整理和其他目的相关的操作。但是，您可能仍然发现
自己需要更加自定义的操作。例如，您可能想要
使用在论文中发现的新颖激活函数，或实现
您在研究中开发的操作。</p>
<p>在 PyTorch 中集成此类自定义操作的最简单方法是通过扩展 
 <code>Function</code>
 和 
 <code>Module</code>
 来用 Python 编写它，如概述
 <a href="https://pytorch.org/docs/master/notes/extending.html">此处</a> 
 。这为您提供了自动微分的全部功能(使您无需编写派生函数)以及 Python 的通常表达能力。然而，有时
您的操作最好用 C++ 实现。例如，您的代码
可能需要
<em>真正</em>
快，因为它在您的模型中被非常频繁地调用
而且即使对于很少的调用也非常昂贵。另一个可能的原因是它依赖于其他 C 或 C++ 库或与其他 C 或 C++ 库交互。为了解决此类情况，
PyTorch 提供了一种非常简单的方法来编写自定义
 <em>C++ 扩展</em> 
 。</p>
<p>C++ 扩展是我们开发的一种机制，允许用户(您)创建
定义的 PyTorch 运算符
 <em>out-of-source</em> 
 ，即与 PyTorch
 后端分开。此方法与本机 PyTorch 操作的实现方式不同。 C++ 扩展旨在为您节省大量与 PyTorch’s 后端集成操作相关的样板文件，同时为您基于 PyTorch 的项目提供高度的灵活性。
不过，一旦您定义了将您的操作作为 C++ 扩展，
将其转换为原生 PyTorch 函数很大程度上是代码组织的问题，
如果您决定将您的操作贡献给上游，
您可以在事后解决这个问题。</p>
<h2 id="_1">动机和示例 <a href="#motivation-and-example" title="永久链接到此标题">¶</a></h2>
<p>本说明的其余部分将介绍编写和使用 C++(和 CUDA)扩展的实际示例。如果您被追赶，或者如果
您在一天结束前没有
’ 完成该操作，有人会解雇您，您可以跳过本节并
直接进入下一节中的实现细节。\ n</p>
<p>让’s 假设你’s 已经想出了一种新的循环单元，你发现它
与现有技术相比具有更优越的性能。此循环单元
 与 LSTM 类似，但不同之处在于它缺少
 <em>遗忘门</em> 
 并使用
 <em>指数线性单元</em> 
 (ELU) 作为其内部激活函数。因为
这个单元永远不会忘记，所以我们’ 将其称为
 <em>LLTM</em> 
 或
 <em>Long-Long-Term-Memory</em> 
 单元。</p>
<p>LLTM 与普通 LSTM 的两种不同之处足够重要
我们可以\xe2\x80\x99t 配置 PyTorch\xe2\x80\x99s
 <code>LSTMCell</code>
 以满足我们的目的，因此我们\xe2\x80 \x99 必须
创建一个自定义单元格。对于这个 \xe2\x80\x93 来说，第一个也是最简单的方法可能在所有情况下都是一个好的第一步 \xe2\x80\x93 是使用 Python 在普通 PyTorch 中实现我们想要的功能。为此，我们需要子类
 <a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(在 PyTorch 中v2.1)"><code>torch.nn.Module</code></a>
 并实现 LLTM 的前向传递。这
看起来像这样：</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a>class LLTM(torch.nn.Module):
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>    def __init__(self, input_features, state_size):
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>        super(LLTM, self).__init__()
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>        self.input_features = input_features
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>        self.state_size = state_size
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a>        # 3 * state_size for input gate, output gate and candidate cell gate.
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a>        # input_features + state_size because we will multiply with [input, h].
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a>        self.weights = torch.nn.Parameter(
<a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a>            torch.empty(3 * state_size, input_features + state_size))
<a id="__codelineno-0-10" name="__codelineno-0-10" href="#__codelineno-0-10"></a>        self.bias = torch.nn.Parameter(torch.empty(3 * state_size))
<a id="__codelineno-0-11" name="__codelineno-0-11" href="#__codelineno-0-11"></a>        self.reset_parameters()
<a id="__codelineno-0-12" name="__codelineno-0-12" href="#__codelineno-0-12"></a>
<a id="__codelineno-0-13" name="__codelineno-0-13" href="#__codelineno-0-13"></a>    def reset_parameters(self):
<a id="__codelineno-0-14" name="__codelineno-0-14" href="#__codelineno-0-14"></a>        stdv = 1.0 / math.sqrt(self.state_size)
<a id="__codelineno-0-15" name="__codelineno-0-15" href="#__codelineno-0-15"></a>        for weight in self.parameters():
<a id="__codelineno-0-16" name="__codelineno-0-16" href="#__codelineno-0-16"></a>            weight.data.uniform_(-stdv, +stdv)
<a id="__codelineno-0-17" name="__codelineno-0-17" href="#__codelineno-0-17"></a>
<a id="__codelineno-0-18" name="__codelineno-0-18" href="#__codelineno-0-18"></a>    def forward(self, input, state):
<a id="__codelineno-0-19" name="__codelineno-0-19" href="#__codelineno-0-19"></a>        old_h, old_cell = state
<a id="__codelineno-0-20" name="__codelineno-0-20" href="#__codelineno-0-20"></a>        X = torch.cat([old_h, input], dim=1)
<a id="__codelineno-0-21" name="__codelineno-0-21" href="#__codelineno-0-21"></a>
<a id="__codelineno-0-22" name="__codelineno-0-22" href="#__codelineno-0-22"></a>        # Compute the input, output and candidate cell gates with one MM.
<a id="__codelineno-0-23" name="__codelineno-0-23" href="#__codelineno-0-23"></a>        gate_weights = F.linear(X, self.weights, self.bias)
<a id="__codelineno-0-24" name="__codelineno-0-24" href="#__codelineno-0-24"></a>        # Split the combined gate weight matrix into its components.
<a id="__codelineno-0-25" name="__codelineno-0-25" href="#__codelineno-0-25"></a>        gates = gate_weights.chunk(3, dim=1)
<a id="__codelineno-0-26" name="__codelineno-0-26" href="#__codelineno-0-26"></a>
<a id="__codelineno-0-27" name="__codelineno-0-27" href="#__codelineno-0-27"></a>        input_gate = torch.sigmoid(gates[0])
<a id="__codelineno-0-28" name="__codelineno-0-28" href="#__codelineno-0-28"></a>        output_gate = torch.sigmoid(gates[1])
<a id="__codelineno-0-29" name="__codelineno-0-29" href="#__codelineno-0-29"></a>        # Here we use an ELU instead of the usual tanh.
<a id="__codelineno-0-30" name="__codelineno-0-30" href="#__codelineno-0-30"></a>        candidate_cell = F.elu(gates[2])
<a id="__codelineno-0-31" name="__codelineno-0-31" href="#__codelineno-0-31"></a>
<a id="__codelineno-0-32" name="__codelineno-0-32" href="#__codelineno-0-32"></a>        # Compute the new cell state.
<a id="__codelineno-0-33" name="__codelineno-0-33" href="#__codelineno-0-33"></a>        new_cell = old_cell + candidate_cell * input_gate
<a id="__codelineno-0-34" name="__codelineno-0-34" href="#__codelineno-0-34"></a>        # Compute the new hidden state and output.
<a id="__codelineno-0-35" name="__codelineno-0-35" href="#__codelineno-0-35"></a>        new_h = torch.tanh(new_cell) * output_gate
<a id="__codelineno-0-36" name="__codelineno-0-36" href="#__codelineno-0-36"></a>
<a id="__codelineno-0-37" name="__codelineno-0-37" href="#__codelineno-0-37"></a>        return new_h, new_cell
</code></pre></div>
<p>然后我们可以按预期使用：</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a>import torch
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a>
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a>X = torch.randn(batch_size, input_features)
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a>h = torch.randn(batch_size, state_size)
<a id="__codelineno-1-5" name="__codelineno-1-5" href="#__codelineno-1-5"></a>C = torch.randn(batch_size, state_size)
<a id="__codelineno-1-6" name="__codelineno-1-6" href="#__codelineno-1-6"></a>
<a id="__codelineno-1-7" name="__codelineno-1-7" href="#__codelineno-1-7"></a>rnn = LLTM(input_features, state_size)
<a id="__codelineno-1-8" name="__codelineno-1-8" href="#__codelineno-1-8"></a>
<a id="__codelineno-1-9" name="__codelineno-1-9" href="#__codelineno-1-9"></a>new_h, new_C = rnn(X, (h, C))
</code></pre></div>
<p>当然，如果可能并且合理的话，您应该使用这种方法来扩展 PyTorch。由于 PyTorch 对其 CPU 和 GPU 操作进行了高度优化的实现，由 <a href="https://developer.nvidia.com/cudnn">NVIDIA cuDNN</a> 等库提供支持，
 <a href="https://software.intel.com/en-us/mkl">Intel MKL</a> 
 或 
 <a href="https://github.com/Maratyszcza/NNPACK">NNPACK</a> 
 ，像上面这样的 PyTorch 代码将通常
足够快。然而，我们也可以看出为什么在某些情况下，
还有进一步改进性能的空间。最明显的原因是 PyTorch 不了解您正在实现的算法。它只知道
您用来组成算法的各个操作。因此，PyTorch
必须一个接一个地单独执行您的操作。由于对操作的实现(或
 <em>kernel</em> 
)的每个
单独调用(可能涉及CUDA内核的启动)都有一定的开销，因此在许多函数调用中，此
开销可能会变得很大。此外，
运行我们代码的 Python 解释器本身也会减慢我们的程序速度。</p>
<p>因此，加快速度的一个明确方法是用 C++(或
CUDA)重写部分内容和
 <em>熔断</em> 
 特定的操作组。融合意味着
将许多函数的实现组合到一个函数中，
这得益于
更少的内核启动以及
我们可以执行的其他优化，
提高了全局数据流的可见性。</p>
<p>让’s 看看如何使用 C++ 扩展来实现 
 <em>fused</em> 
 版本的
LLTM。我们’ 将首先使用普通 C++ 编写它，使用
 <a href="https://github.com/zdevito/ATen">ATen</a>
 库，该库为 PyTorch’s\ 的大部分提供支持
 nbackend，看看它如何让我们轻松地翻译 Python 代码。然后，我们’
将模型的部分内容移至 CUDA 内核，
从 GPU 提供的大规模并行性中获益，从而进一步加快速度。</p>
<h2 id="c">编写 C++ 扩展 <a href="#writing-a-c-extension" title="永久链接到此标题">¶</a></h2>
<p>C++ 扩展有两种风格：它们可以使用
 <code>setuptools</code>
 提前\xe2\x80\x9c 构建\xe2\x80\x9c，或者及时\xe2\x80\ 构建\xe2\x80\x9c x9d via
 <a href="https://pytorch.org/docs/stable/cpp_extension.html#torch.utils.cpp_extension.load" title="(在 PyTorch v2.1 中) 1)"><code>torch.utils.cpp_extension.load()</code></a>
.我们\xe2\x80\x99将从第一种方法开始，
稍后讨论后者。</p>
<h3 id="_2">使用</h3>
<p><code>setuptools</code>构建 <a href="#building-with-setuptools" title="永久链接到此标题">¶</a></p>
<p>对于“ahead of time” 风格，我们通过编写
 <code>setup.py</code>
 脚本来构建我们的C++ 扩展，该脚本使用setuptools 来编译我们的C++ 代码。对于 LLTM，
看起来就像这样简单：</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a>from setuptools import setup, Extension
<a id="__codelineno-2-2" name="__codelineno-2-2" href="#__codelineno-2-2"></a>from torch.utils import cpp_extension
<a id="__codelineno-2-3" name="__codelineno-2-3" href="#__codelineno-2-3"></a>
<a id="__codelineno-2-4" name="__codelineno-2-4" href="#__codelineno-2-4"></a>setup(name=&#39;lltm_cpp&#39;,
<a id="__codelineno-2-5" name="__codelineno-2-5" href="#__codelineno-2-5"></a>      ext_modules=[cpp_extension.CppExtension(&#39;lltm_cpp&#39;, [&#39;lltm.cpp&#39;])],
<a id="__codelineno-2-6" name="__codelineno-2-6" href="#__codelineno-2-6"></a>      cmdclass={&#39;build_ext&#39;: cpp_extension.BuildExtension})
</code></pre></div>
<p>在此代码中，
 <code>CppExtension</code>
 是 <code>setuptools.Extension</code>
 的便捷包装器，它传递正确的包含路径并将
扩展语言设置为 C++。等效的普通
 <code>setuptools</code>
 代码就是：</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1" href="#__codelineno-3-1"></a>Extension(
<a id="__codelineno-3-2" name="__codelineno-3-2" href="#__codelineno-3-2"></a>   name=&#39;lltm_cpp&#39;,
<a id="__codelineno-3-3" name="__codelineno-3-3" href="#__codelineno-3-3"></a>   sources=[&#39;lltm.cpp&#39;],
<a id="__codelineno-3-4" name="__codelineno-3-4" href="#__codelineno-3-4"></a>   include_dirs=cpp_extension.include_paths(),
<a id="__codelineno-3-5" name="__codelineno-3-5" href="#__codelineno-3-5"></a>   language=&#39;c++&#39;)
</code></pre></div>
<p><code>BuildExtension</code>
 执行许多必需的配置步骤和检查，并在混合 C++/CUDA
 扩展的情况下管理混合编译。 ’ 是我们现在真正需要了解的关于构建 C++ 扩展的全部信息！现在让’s 看一下我们的 C++ 扩展的实现，
它进入
 <code>lltm.cpp</code>
 。</p>
<h3 id="c-op">编写 C++ Op <a href="#writing-the-c-op" title="此标题的永久链接">¶</a></h3>
<p>让’s 开始用C++ 实现LLTM！我们’ 向后传递所需的函数之一是 sigmoid 的导数。这是一段足够小的代码，
足以讨论我们在编写 C++
扩展时可用的整体环境：</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-4-1" name="__codelineno-4-1" href="#__codelineno-4-1"></a>#include &lt;torch/extension.h&gt;
<a id="__codelineno-4-2" name="__codelineno-4-2" href="#__codelineno-4-2"></a>
<a id="__codelineno-4-3" name="__codelineno-4-3" href="#__codelineno-4-3"></a>#include &lt;iostream&gt;
<a id="__codelineno-4-4" name="__codelineno-4-4" href="#__codelineno-4-4"></a>
<a id="__codelineno-4-5" name="__codelineno-4-5" href="#__codelineno-4-5"></a>torch::Tensor d_sigmoid(torch::Tensor z) {
<a id="__codelineno-4-6" name="__codelineno-4-6" href="#__codelineno-4-6"></a> auto s = torch::sigmoid(z);
<a id="__codelineno-4-7" name="__codelineno-4-7" href="#__codelineno-4-7"></a> return (1 - s) * s;
<a id="__codelineno-4-8" name="__codelineno-4-8" href="#__codelineno-4-8"></a>}
</code></pre></div>
<p><code>&lt;torch/extension.h&gt;</code>
 是一站式标头，包含编写 C++ 扩展所需的所有 PyTorch
 位。它包括：</p>
<ul>
<li>ATen 库，这是我们用于tensor计算的主要 API，</li>
<li><a href="https://github.com/pybind/pybind11">pybind11</a> 
，这是我们为 C++ 代码创建 Python 绑定的方式，</li>
<li>管理 ATen 和 pybind11 之间交互细节的标头。</li>
</ul>
<p><code>d_sigmoid()</code>的实现展示了如何使用 ATen API。
PyTorch’s tensor和变量接口是从 ATen 库自动生成的，因此我们可以更多或者更少地将我们的 Python 实现 1:1
 转换为 C++。我们所有计算的主要数据类型将是
 <code>torch::Tensor</code>
 。可以在<a href="https://pytorch.org/cppdocs/api/classat_1_1_tensor.html">此处</a>检查其完整 API。另请注意，我们可以包含
 <code>&lt;iostream&gt;</code>
 或
 <em>任何其他 C 或 C++ 标头</em> 
 –，我们可以使用
C++11 的全部功能。 </p>
<p>请注意，在 Windows 上解析 torch/extension.h 时，CUDA-11.5 nvcc 将遇到内部编译器错误。
要解决此问题，请将 python 绑定逻辑移至纯 C++ 文件。
使用示例:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-5-1" name="__codelineno-5-1" href="#__codelineno-5-1"></a>#include &lt;ATen/ATen.h&gt;
<a id="__codelineno-5-2" name="__codelineno-5-2" href="#__codelineno-5-2"></a>at::Tensor SigmoidAlphaBlendForwardCuda(....)
</code></pre></div>
<p>而不是：</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-6-1" name="__codelineno-6-1" href="#__codelineno-6-1"></a>#include &lt;torch/extension.h&gt;
<a id="__codelineno-6-2" name="__codelineno-6-2" href="#__codelineno-6-2"></a>torch::Tensor SigmoidAlphaBlendForwardCuda(...)
</code></pre></div>
<p>当前未解决的 nvcc bug 问题
 <a href="https://github.com/pytorch/pytorch/issues/69460">此处</a> 
.
完整的解决方法代码示例
 <a href="https://github.com/facebookresearch/pytorch3d/commit/cb170ac024a949f1f9614ffe6af1c38d972f7d48">此处</a> 
.</p>
<h4 id="_3">正向传递 <a href="#forward-pass" title="永久链接到此标题">¶</a></h4>
<p>接下来我们可以将整个前向传递移植到 C++：</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-7-1" name="__codelineno-7-1" href="#__codelineno-7-1"></a>#include &lt;vector&gt;
<a id="__codelineno-7-2" name="__codelineno-7-2" href="#__codelineno-7-2"></a>
<a id="__codelineno-7-3" name="__codelineno-7-3" href="#__codelineno-7-3"></a>std::vector&lt;at::Tensor&gt; lltm_forward(
<a id="__codelineno-7-4" name="__codelineno-7-4" href="#__codelineno-7-4"></a> torch::Tensor input,
<a id="__codelineno-7-5" name="__codelineno-7-5" href="#__codelineno-7-5"></a> torch::Tensor weights,
<a id="__codelineno-7-6" name="__codelineno-7-6" href="#__codelineno-7-6"></a> torch::Tensor bias,
<a id="__codelineno-7-7" name="__codelineno-7-7" href="#__codelineno-7-7"></a> torch::Tensor old_h,
<a id="__codelineno-7-8" name="__codelineno-7-8" href="#__codelineno-7-8"></a> torch::Tensor old_cell) {
<a id="__codelineno-7-9" name="__codelineno-7-9" href="#__codelineno-7-9"></a> auto X = torch::cat({old_h, input}, /*dim=*/1);
<a id="__codelineno-7-10" name="__codelineno-7-10" href="#__codelineno-7-10"></a>
<a id="__codelineno-7-11" name="__codelineno-7-11" href="#__codelineno-7-11"></a> auto gate_weights = torch::addmm(bias, X, weights.transpose(0, 1));
<a id="__codelineno-7-12" name="__codelineno-7-12" href="#__codelineno-7-12"></a> auto gates = gate_weights.chunk(3, /*dim=*/1);
<a id="__codelineno-7-13" name="__codelineno-7-13" href="#__codelineno-7-13"></a>
<a id="__codelineno-7-14" name="__codelineno-7-14" href="#__codelineno-7-14"></a> auto input_gate = torch::sigmoid(gates[0]);
<a id="__codelineno-7-15" name="__codelineno-7-15" href="#__codelineno-7-15"></a> auto output_gate = torch::sigmoid(gates[1]);
<a id="__codelineno-7-16" name="__codelineno-7-16" href="#__codelineno-7-16"></a> auto candidate_cell = torch::elu(gates[2], /*alpha=*/1.0);
<a id="__codelineno-7-17" name="__codelineno-7-17" href="#__codelineno-7-17"></a>
<a id="__codelineno-7-18" name="__codelineno-7-18" href="#__codelineno-7-18"></a> auto new_cell = old_cell + candidate_cell * input_gate;
<a id="__codelineno-7-19" name="__codelineno-7-19" href="#__codelineno-7-19"></a> auto new_h = torch::tanh(new_cell) * output_gate;
<a id="__codelineno-7-20" name="__codelineno-7-20" href="#__codelineno-7-20"></a>
<a id="__codelineno-7-21" name="__codelineno-7-21" href="#__codelineno-7-21"></a> return {new_h,
<a id="__codelineno-7-22" name="__codelineno-7-22" href="#__codelineno-7-22"></a> new_cell,
<a id="__codelineno-7-23" name="__codelineno-7-23" href="#__codelineno-7-23"></a> input_gate,
<a id="__codelineno-7-24" name="__codelineno-7-24" href="#__codelineno-7-24"></a> output_gate,
<a id="__codelineno-7-25" name="__codelineno-7-25" href="#__codelineno-7-25"></a> candidate_cell,
<a id="__codelineno-7-26" name="__codelineno-7-26" href="#__codelineno-7-26"></a> X,
<a id="__codelineno-7-27" name="__codelineno-7-27" href="#__codelineno-7-27"></a> gate_weights};
<a id="__codelineno-7-28" name="__codelineno-7-28" href="#__codelineno-7-28"></a>}
</code></pre></div>
<h4 id="_4">向后传递 <a href="#backward-pass" title="永久链接到此标题">¶</a></h4>
<p>C++ 扩展 API 目前不提供自动
为我们生成向后函数的方法。因此，我们还必须实现 LLTM 的后向传递，它计算损失相对于前向传递的每个输入的导数。最终，我们将
forward和backward函数放入
 <a href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function" title="(in PyTorch v2.1)"><code>torch.autograd.Function</code></a>
 创建
 良好的 Python 绑定。向后函数稍微复杂一些，所以
我们’不会深入研究代码(如果你有兴趣，
 <a href="https://www.cs.toronto.edu/~graves/phd.pdf">Alex Graves’ 论文</a>
 是一本很好的读物，可了解更多
这方面的信息)：</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-8-1" name="__codelineno-8-1" href="#__codelineno-8-1"></a>// tanh&#39;(z) = 1 - tanh^2(z)
<a id="__codelineno-8-2" name="__codelineno-8-2" href="#__codelineno-8-2"></a>torch::Tensor d_tanh(torch::Tensor z) {
<a id="__codelineno-8-3" name="__codelineno-8-3" href="#__codelineno-8-3"></a> return 1 - z.tanh().pow(2);
<a id="__codelineno-8-4" name="__codelineno-8-4" href="#__codelineno-8-4"></a>}
<a id="__codelineno-8-5" name="__codelineno-8-5" href="#__codelineno-8-5"></a>
<a id="__codelineno-8-6" name="__codelineno-8-6" href="#__codelineno-8-6"></a>// elu&#39;(z) = relu&#39;(z) + { alpha * exp(z) if (alpha * (exp(z) - 1)) &lt; 0, else 0}
<a id="__codelineno-8-7" name="__codelineno-8-7" href="#__codelineno-8-7"></a>torch::Tensor d_elu(torch::Tensor z, torch::Scalar alpha = 1.0) {
<a id="__codelineno-8-8" name="__codelineno-8-8" href="#__codelineno-8-8"></a> auto e = z.exp();
<a id="__codelineno-8-9" name="__codelineno-8-9" href="#__codelineno-8-9"></a> auto mask = (alpha * (e - 1)) &lt; 0;
<a id="__codelineno-8-10" name="__codelineno-8-10" href="#__codelineno-8-10"></a> return (z &gt; 0).type_as(z) + mask.type_as(z) * (alpha * e);
<a id="__codelineno-8-11" name="__codelineno-8-11" href="#__codelineno-8-11"></a>}
<a id="__codelineno-8-12" name="__codelineno-8-12" href="#__codelineno-8-12"></a>
<a id="__codelineno-8-13" name="__codelineno-8-13" href="#__codelineno-8-13"></a>std::vector&lt;torch::Tensor&gt; lltm_backward(
<a id="__codelineno-8-14" name="__codelineno-8-14" href="#__codelineno-8-14"></a> torch::Tensor grad_h,
<a id="__codelineno-8-15" name="__codelineno-8-15" href="#__codelineno-8-15"></a> torch::Tensor grad_cell,
<a id="__codelineno-8-16" name="__codelineno-8-16" href="#__codelineno-8-16"></a> torch::Tensor new_cell,
<a id="__codelineno-8-17" name="__codelineno-8-17" href="#__codelineno-8-17"></a> torch::Tensor input_gate,
<a id="__codelineno-8-18" name="__codelineno-8-18" href="#__codelineno-8-18"></a> torch::Tensor output_gate,
<a id="__codelineno-8-19" name="__codelineno-8-19" href="#__codelineno-8-19"></a> torch::Tensor candidate_cell,
<a id="__codelineno-8-20" name="__codelineno-8-20" href="#__codelineno-8-20"></a> torch::Tensor X,
<a id="__codelineno-8-21" name="__codelineno-8-21" href="#__codelineno-8-21"></a> torch::Tensor gate_weights,
<a id="__codelineno-8-22" name="__codelineno-8-22" href="#__codelineno-8-22"></a> torch::Tensor weights) {
<a id="__codelineno-8-23" name="__codelineno-8-23" href="#__codelineno-8-23"></a> auto d_output_gate = torch::tanh(new_cell) * grad_h;
<a id="__codelineno-8-24" name="__codelineno-8-24" href="#__codelineno-8-24"></a> auto d_tanh_new_cell = output_gate * grad_h;
<a id="__codelineno-8-25" name="__codelineno-8-25" href="#__codelineno-8-25"></a> auto d_new_cell = d_tanh(new_cell) * d_tanh_new_cell + grad_cell;
<a id="__codelineno-8-26" name="__codelineno-8-26" href="#__codelineno-8-26"></a>
<a id="__codelineno-8-27" name="__codelineno-8-27" href="#__codelineno-8-27"></a> auto d_old_cell = d_new_cell;
<a id="__codelineno-8-28" name="__codelineno-8-28" href="#__codelineno-8-28"></a> auto d_candidate_cell = input_gate * d_new_cell;
<a id="__codelineno-8-29" name="__codelineno-8-29" href="#__codelineno-8-29"></a> auto d_input_gate = candidate_cell * d_new_cell;
<a id="__codelineno-8-30" name="__codelineno-8-30" href="#__codelineno-8-30"></a>
<a id="__codelineno-8-31" name="__codelineno-8-31" href="#__codelineno-8-31"></a> auto gates = gate_weights.chunk(3, /*dim=*/1);
<a id="__codelineno-8-32" name="__codelineno-8-32" href="#__codelineno-8-32"></a> d_input_gate *= d_sigmoid(gates[0]);
<a id="__codelineno-8-33" name="__codelineno-8-33" href="#__codelineno-8-33"></a> d_output_gate *= d_sigmoid(gates[1]);
<a id="__codelineno-8-34" name="__codelineno-8-34" href="#__codelineno-8-34"></a> d_candidate_cell *= d_elu(gates[2]);
<a id="__codelineno-8-35" name="__codelineno-8-35" href="#__codelineno-8-35"></a>
<a id="__codelineno-8-36" name="__codelineno-8-36" href="#__codelineno-8-36"></a> auto d_gates =
<a id="__codelineno-8-37" name="__codelineno-8-37" href="#__codelineno-8-37"></a> torch::cat({d_input_gate, d_output_gate, d_candidate_cell}, /*dim=*/1);
<a id="__codelineno-8-38" name="__codelineno-8-38" href="#__codelineno-8-38"></a>
<a id="__codelineno-8-39" name="__codelineno-8-39" href="#__codelineno-8-39"></a> auto d_weights = d_gates.t().mm(X);
<a id="__codelineno-8-40" name="__codelineno-8-40" href="#__codelineno-8-40"></a> auto d_bias = d_gates.sum(/*dim=*/0, /*keepdim=*/true);
<a id="__codelineno-8-41" name="__codelineno-8-41" href="#__codelineno-8-41"></a>
<a id="__codelineno-8-42" name="__codelineno-8-42" href="#__codelineno-8-42"></a> auto d_X = d_gates.mm(weights);
<a id="__codelineno-8-43" name="__codelineno-8-43" href="#__codelineno-8-43"></a> const auto state_size = grad_h.size(1);
<a id="__codelineno-8-44" name="__codelineno-8-44" href="#__codelineno-8-44"></a> auto d_old_h = d_X.slice(/*dim=*/1, 0, state_size);
<a id="__codelineno-8-45" name="__codelineno-8-45" href="#__codelineno-8-45"></a> auto d_input = d_X.slice(/*dim=*/1, state_size);
<a id="__codelineno-8-46" name="__codelineno-8-46" href="#__codelineno-8-46"></a>
<a id="__codelineno-8-47" name="__codelineno-8-47" href="#__codelineno-8-47"></a> return {d_old_h, d_input, d_weights, d_bias, d_old_cell};
<a id="__codelineno-8-48" name="__codelineno-8-48" href="#__codelineno-8-48"></a>}
</code></pre></div>
<h3 id="python">绑定到 Python <a href="#binding-to-python" title="永久链接到此标题">¶</a></h3>
<p>一旦您用 C++ 和 ATen 编写了操作，您就可以使用 pybind11 以非常简单的方式将您的 C++ 函数或类绑定到 Python 中。
您对 PyTorch C++ 扩展的这一部分的疑问或问题将主要是
由
 <a href="https://pybind11.readthedocs.io/en/stable/">pybind11 文档</a> 解决
 。</p>
<p>对于我们的扩展，必要的绑定代码仅跨越四行：</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-9-1" name="__codelineno-9-1" href="#__codelineno-9-1"></a>PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
<a id="__codelineno-9-2" name="__codelineno-9-2" href="#__codelineno-9-2"></a> m.def(&quot;forward&quot;, &amp;lltm_forward, &quot;LLTM forward&quot;);
<a id="__codelineno-9-3" name="__codelineno-9-3" href="#__codelineno-9-3"></a> m.def(&quot;backward&quot;, &amp;lltm_backward, &quot;LLTM backward&quot;);
<a id="__codelineno-9-4" name="__codelineno-9-4" href="#__codelineno-9-4"></a>}
</code></pre></div>
<p>这里需要注意的一点是宏
 <code>TORCH_EXTENSION_NAME</code>
 。 torch 扩展
build 会将其定义为您在
 <code>setup.py</code>
 脚本中为扩展指定的名称。在这种情况下，
 <code>TORCH_EXTENSION_NAME</code>
 的值将为 “lltm_cpp”。
这是为了避免必须维护
在两个地方(构建脚本和 C++ 代码)进行扩展，因为两者之间的不匹配可能导致
令人讨厌且难以跟踪的问题。</p>
<h3 id="_5">使用您的扩展 <a href="#using-your-extension" title="永久链接到此标题">¶</a></h3>
<p>我们现在准备在 PyTorch 中导入我们的扩展。此时，您的目录
结构可能如下所示：</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-10-1" name="__codelineno-10-1" href="#__codelineno-10-1"></a>pytorch/
<a id="__codelineno-10-2" name="__codelineno-10-2" href="#__codelineno-10-2"></a>  lltm-extension/
<a id="__codelineno-10-3" name="__codelineno-10-3" href="#__codelineno-10-3"></a>    lltm.cpp
<a id="__codelineno-10-4" name="__codelineno-10-4" href="#__codelineno-10-4"></a>    setup.py
</code></pre></div>
<p>现在，运行
 `python</p>
<p>setup.py</p>
<p>install`
 来构建并安装您的扩展。这
应该看起来像这样：</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-11-1" name="__codelineno-11-1" href="#__codelineno-11-1"></a>running install
<a id="__codelineno-11-2" name="__codelineno-11-2" href="#__codelineno-11-2"></a>running bdist_egg
<a id="__codelineno-11-3" name="__codelineno-11-3" href="#__codelineno-11-3"></a>running egg_info
<a id="__codelineno-11-4" name="__codelineno-11-4" href="#__codelineno-11-4"></a>creating lltm_cpp.egg-info
<a id="__codelineno-11-5" name="__codelineno-11-5" href="#__codelineno-11-5"></a>writing lltm_cpp.egg-info/PKG-INFO
<a id="__codelineno-11-6" name="__codelineno-11-6" href="#__codelineno-11-6"></a>writing dependency_links to lltm_cpp.egg-info/dependency_links.txt
<a id="__codelineno-11-7" name="__codelineno-11-7" href="#__codelineno-11-7"></a>writing top-level names to lltm_cpp.egg-info/top_level.txt
<a id="__codelineno-11-8" name="__codelineno-11-8" href="#__codelineno-11-8"></a>writing manifest file &#39;lltm_cpp.egg-info/SOURCES.txt&#39;
<a id="__codelineno-11-9" name="__codelineno-11-9" href="#__codelineno-11-9"></a>reading manifest file &#39;lltm_cpp.egg-info/SOURCES.txt&#39;
<a id="__codelineno-11-10" name="__codelineno-11-10" href="#__codelineno-11-10"></a>writing manifest file &#39;lltm_cpp.egg-info/SOURCES.txt&#39;
<a id="__codelineno-11-11" name="__codelineno-11-11" href="#__codelineno-11-11"></a>installing library code to build/bdist.linux-x86_64/egg
<a id="__codelineno-11-12" name="__codelineno-11-12" href="#__codelineno-11-12"></a>running install_lib
<a id="__codelineno-11-13" name="__codelineno-11-13" href="#__codelineno-11-13"></a>running build_ext
<a id="__codelineno-11-14" name="__codelineno-11-14" href="#__codelineno-11-14"></a>building &#39;lltm_cpp&#39; extension
<a id="__codelineno-11-15" name="__codelineno-11-15" href="#__codelineno-11-15"></a>creating build
<a id="__codelineno-11-16" name="__codelineno-11-16" href="#__codelineno-11-16"></a>creating build/temp.linux-x86_64-3.7
<a id="__codelineno-11-17" name="__codelineno-11-17" href="#__codelineno-11-17"></a>gcc -pthread -B ~/local/miniconda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I~/local/miniconda/lib/python3.7/site-packages/torch/include -I~/local/miniconda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -I~/local/miniconda/lib/python3.7/site-packages/torch/include/TH -I~/local/miniconda/lib/python3.7/site-packages/torch/include/THC -I~/local/miniconda/include/python3.7m -c lltm.cpp -o build/temp.linux-x86_64-3.7/lltm.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=lltm_cpp -D_GLIBCXX_USE_CXX11_ABI=1 -std=c++11
<a id="__codelineno-11-18" name="__codelineno-11-18" href="#__codelineno-11-18"></a>cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++
<a id="__codelineno-11-19" name="__codelineno-11-19" href="#__codelineno-11-19"></a>creating build/lib.linux-x86_64-3.7
<a id="__codelineno-11-20" name="__codelineno-11-20" href="#__codelineno-11-20"></a>g++ -pthread -shared -B ~/local/miniconda/compiler_compat -L~/local/miniconda/lib -Wl,-rpath=~/local/miniconda/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.7/lltm.o -o build/lib.linux-x86_64-3.7/lltm_cpp.cpython-37m-x86_64-linux-gnu.so
<a id="__codelineno-11-21" name="__codelineno-11-21" href="#__codelineno-11-21"></a>creating build/bdist.linux-x86_64
<a id="__codelineno-11-22" name="__codelineno-11-22" href="#__codelineno-11-22"></a>creating build/bdist.linux-x86_64/egg
<a id="__codelineno-11-23" name="__codelineno-11-23" href="#__codelineno-11-23"></a>copying build/lib.linux-x86_64-3.7/lltm_cpp.cpython-37m-x86_64-linux-gnu.so -&gt; build/bdist.linux-x86_64/egg
<a id="__codelineno-11-24" name="__codelineno-11-24" href="#__codelineno-11-24"></a>creating stub loader for lltm_cpp.cpython-37m-x86_64-linux-gnu.so
<a id="__codelineno-11-25" name="__codelineno-11-25" href="#__codelineno-11-25"></a>byte-compiling build/bdist.linux-x86_64/egg/lltm_cpp.py to lltm_cpp.cpython-37.pyc
<a id="__codelineno-11-26" name="__codelineno-11-26" href="#__codelineno-11-26"></a>creating build/bdist.linux-x86_64/egg/EGG-INFO
<a id="__codelineno-11-27" name="__codelineno-11-27" href="#__codelineno-11-27"></a>copying lltm_cpp.egg-info/PKG-INFO -&gt; build/bdist.linux-x86_64/egg/EGG-INFO
<a id="__codelineno-11-28" name="__codelineno-11-28" href="#__codelineno-11-28"></a>copying lltm_cpp.egg-info/SOURCES.txt -&gt; build/bdist.linux-x86_64/egg/EGG-INFO
<a id="__codelineno-11-29" name="__codelineno-11-29" href="#__codelineno-11-29"></a>copying lltm_cpp.egg-info/dependency_links.txt -&gt; build/bdist.linux-x86_64/egg/EGG-INFO
<a id="__codelineno-11-30" name="__codelineno-11-30" href="#__codelineno-11-30"></a>copying lltm_cpp.egg-info/top_level.txt -&gt; build/bdist.linux-x86_64/egg/EGG-INFO
<a id="__codelineno-11-31" name="__codelineno-11-31" href="#__codelineno-11-31"></a>writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt
<a id="__codelineno-11-32" name="__codelineno-11-32" href="#__codelineno-11-32"></a>zip_safe flag not set; analyzing archive contents...
<a id="__codelineno-11-33" name="__codelineno-11-33" href="#__codelineno-11-33"></a>__pycache__.lltm_cpp.cpython-37: module references __file__
<a id="__codelineno-11-34" name="__codelineno-11-34" href="#__codelineno-11-34"></a>creating &#39;dist/lltm_cpp-0.0.0-py3.7-linux-x86_64.egg&#39; and adding &#39;build/bdist.linux-x86_64/egg&#39; to it
<a id="__codelineno-11-35" name="__codelineno-11-35" href="#__codelineno-11-35"></a>removing &#39;build/bdist.linux-x86_64/egg&#39; (and everything under it)
<a id="__codelineno-11-36" name="__codelineno-11-36" href="#__codelineno-11-36"></a>Processing lltm_cpp-0.0.0-py3.7-linux-x86_64.egg
<a id="__codelineno-11-37" name="__codelineno-11-37" href="#__codelineno-11-37"></a>removing &#39;~/local/miniconda/lib/python3.7/site-packages/lltm_cpp-0.0.0-py3.7-linux-x86_64.egg&#39; (and everything under it)
<a id="__codelineno-11-38" name="__codelineno-11-38" href="#__codelineno-11-38"></a>creating ~/local/miniconda/lib/python3.7/site-packages/lltm_cpp-0.0.0-py3.7-linux-x86_64.egg
<a id="__codelineno-11-39" name="__codelineno-11-39" href="#__codelineno-11-39"></a>Extracting lltm_cpp-0.0.0-py3.7-linux-x86_64.egg to ~/local/miniconda/lib/python3.7/site-packages
<a id="__codelineno-11-40" name="__codelineno-11-40" href="#__codelineno-11-40"></a>lltm-cpp 0.0.0 is already the active version in easy-install.pth
<a id="__codelineno-11-41" name="__codelineno-11-41" href="#__codelineno-11-41"></a>
<a id="__codelineno-11-42" name="__codelineno-11-42" href="#__codelineno-11-42"></a>Installed ~/local/miniconda/lib/python3.7/site-packages/lltm_cpp-0.0.0-py3.7-linux-x86_64.egg
<a id="__codelineno-11-43" name="__codelineno-11-43" href="#__codelineno-11-43"></a>Processing dependencies for lltm-cpp==0.0.0
<a id="__codelineno-11-44" name="__codelineno-11-44" href="#__codelineno-11-44"></a>Finished processing dependencies for lltm-cpp==0.0.0
</code></pre></div>
<p>关于编译器的一个小注意事项：由于 ABI 版本控制问题，用于构建 C++ 扩展的编译器必须
 <em>ABI 兼容</em> 
 与构建 PyTorch 时
所使用的编译器
相同。实际上，这意味着您必须在 Linux 上使用 GCC 版本 4.9 及更高版本。
对于 Ubuntu 16.04 和其他更新的 Linux 发行版，这应该已经是
默认编译器。在 MacOS 上，您必须使用 clang(它没有任何 ABI 版本控制问题)。在最坏的情况下，您可以使用编译器从源代码构建 PyTorch，然后使用同一编译器构建扩展。</p>
<p>构建扩展后，您只需使用在 
 <code>setup.py</code> 脚本中指定的
名称将其导入到 Python 中即可。请务必先
 `import</p>
<p>torch`
，因为这将解析动态链接器必须
看到的一些符号：</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-12-1" name="__codelineno-12-1" href="#__codelineno-12-1"></a>In [1]: import torch
<a id="__codelineno-12-2" name="__codelineno-12-2" href="#__codelineno-12-2"></a>In [2]: import lltm_cpp
<a id="__codelineno-12-3" name="__codelineno-12-3" href="#__codelineno-12-3"></a>In [3]: lltm_cpp.forward
<a id="__codelineno-12-4" name="__codelineno-12-4" href="#__codelineno-12-4"></a>Out[3]: &lt;function lltm.PyCapsule.forward&gt;
</code></pre></div>
<p>如果我们在函数或模块上调用
 <code>help()</code>
，我们可以看到它的签名
与我们的 C++ 代码匹配：</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-13-1" name="__codelineno-13-1" href="#__codelineno-13-1"></a>In[4] help(lltm_cpp.forward)
<a id="__codelineno-13-2" name="__codelineno-13-2" href="#__codelineno-13-2"></a>forward(...) method of builtins.PyCapsule instance
<a id="__codelineno-13-3" name="__codelineno-13-3" href="#__codelineno-13-3"></a>    forward(arg0: torch::Tensor, arg1: torch::Tensor, arg2: torch::Tensor, arg3: torch::Tensor, arg4: torch::Tensor) -&gt; List[torch::Tensor]
<a id="__codelineno-13-4" name="__codelineno-13-4" href="#__codelineno-13-4"></a>
<a id="__codelineno-13-5" name="__codelineno-13-5" href="#__codelineno-13-5"></a>    LLTM forward
</code></pre></div>
<p>由于我们现在可以从 Python 调用 C++ 函数，因此我们可以用 <a href="https://pytorch.org/docs/stable/autograd.html 包装它们
 #torch.autograd.Function" title="(在 PyTorch v2.1 中)"><code>torch.autograd.Function</code></a>
 和 
 <a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(在 PyTorch v2.1)"><code>torch.nn.Module</code></a>
 使它们成为 PyTorch 的一等公民：</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-14-1" name="__codelineno-14-1" href="#__codelineno-14-1"></a>import math
<a id="__codelineno-14-2" name="__codelineno-14-2" href="#__codelineno-14-2"></a>import torch
<a id="__codelineno-14-3" name="__codelineno-14-3" href="#__codelineno-14-3"></a>
<a id="__codelineno-14-4" name="__codelineno-14-4" href="#__codelineno-14-4"></a># Our module!
<a id="__codelineno-14-5" name="__codelineno-14-5" href="#__codelineno-14-5"></a>import lltm_cpp
<a id="__codelineno-14-6" name="__codelineno-14-6" href="#__codelineno-14-6"></a>
<a id="__codelineno-14-7" name="__codelineno-14-7" href="#__codelineno-14-7"></a>class LLTMFunction(torch.autograd.Function):
<a id="__codelineno-14-8" name="__codelineno-14-8" href="#__codelineno-14-8"></a>    @staticmethod
<a id="__codelineno-14-9" name="__codelineno-14-9" href="#__codelineno-14-9"></a>    def forward(ctx, input, weights, bias, old_h, old_cell):
<a id="__codelineno-14-10" name="__codelineno-14-10" href="#__codelineno-14-10"></a>        outputs = lltm_cpp.forward(input, weights, bias, old_h, old_cell)
<a id="__codelineno-14-11" name="__codelineno-14-11" href="#__codelineno-14-11"></a>        new_h, new_cell = outputs[:2]
<a id="__codelineno-14-12" name="__codelineno-14-12" href="#__codelineno-14-12"></a>        variables = outputs[1:] + [weights]
<a id="__codelineno-14-13" name="__codelineno-14-13" href="#__codelineno-14-13"></a>        ctx.save_for_backward(*variables)
<a id="__codelineno-14-14" name="__codelineno-14-14" href="#__codelineno-14-14"></a>
<a id="__codelineno-14-15" name="__codelineno-14-15" href="#__codelineno-14-15"></a>        return new_h, new_cell
<a id="__codelineno-14-16" name="__codelineno-14-16" href="#__codelineno-14-16"></a>
<a id="__codelineno-14-17" name="__codelineno-14-17" href="#__codelineno-14-17"></a>    @staticmethod
<a id="__codelineno-14-18" name="__codelineno-14-18" href="#__codelineno-14-18"></a>    def backward(ctx, grad_h, grad_cell):
<a id="__codelineno-14-19" name="__codelineno-14-19" href="#__codelineno-14-19"></a>        outputs = lltm_cpp.backward(
<a id="__codelineno-14-20" name="__codelineno-14-20" href="#__codelineno-14-20"></a>            grad_h.contiguous(), grad_cell.contiguous(), *ctx.saved_tensors)
<a id="__codelineno-14-21" name="__codelineno-14-21" href="#__codelineno-14-21"></a>        d_old_h, d_input, d_weights, d_bias, d_old_cell = outputs
<a id="__codelineno-14-22" name="__codelineno-14-22" href="#__codelineno-14-22"></a>        return d_input, d_weights, d_bias, d_old_h, d_old_cell
<a id="__codelineno-14-23" name="__codelineno-14-23" href="#__codelineno-14-23"></a>
<a id="__codelineno-14-24" name="__codelineno-14-24" href="#__codelineno-14-24"></a>
<a id="__codelineno-14-25" name="__codelineno-14-25" href="#__codelineno-14-25"></a>class LLTM(torch.nn.Module):
<a id="__codelineno-14-26" name="__codelineno-14-26" href="#__codelineno-14-26"></a>    def __init__(self, input_features, state_size):
<a id="__codelineno-14-27" name="__codelineno-14-27" href="#__codelineno-14-27"></a>        super(LLTM, self).__init__()
<a id="__codelineno-14-28" name="__codelineno-14-28" href="#__codelineno-14-28"></a>        self.input_features = input_features
<a id="__codelineno-14-29" name="__codelineno-14-29" href="#__codelineno-14-29"></a>        self.state_size = state_size
<a id="__codelineno-14-30" name="__codelineno-14-30" href="#__codelineno-14-30"></a>        self.weights = torch.nn.Parameter(
<a id="__codelineno-14-31" name="__codelineno-14-31" href="#__codelineno-14-31"></a>            torch.empty(3 * state_size, input_features + state_size))
<a id="__codelineno-14-32" name="__codelineno-14-32" href="#__codelineno-14-32"></a>        self.bias = torch.nn.Parameter(torch.empty(3 * state_size))
<a id="__codelineno-14-33" name="__codelineno-14-33" href="#__codelineno-14-33"></a>        self.reset_parameters()
<a id="__codelineno-14-34" name="__codelineno-14-34" href="#__codelineno-14-34"></a>
<a id="__codelineno-14-35" name="__codelineno-14-35" href="#__codelineno-14-35"></a>    def reset_parameters(self):
<a id="__codelineno-14-36" name="__codelineno-14-36" href="#__codelineno-14-36"></a>        stdv = 1.0 / math.sqrt(self.state_size)
<a id="__codelineno-14-37" name="__codelineno-14-37" href="#__codelineno-14-37"></a>        for weight in self.parameters():
<a id="__codelineno-14-38" name="__codelineno-14-38" href="#__codelineno-14-38"></a>            weight.data.uniform_(-stdv, +stdv)
<a id="__codelineno-14-39" name="__codelineno-14-39" href="#__codelineno-14-39"></a>
<a id="__codelineno-14-40" name="__codelineno-14-40" href="#__codelineno-14-40"></a>    def forward(self, input, state):
<a id="__codelineno-14-41" name="__codelineno-14-41" href="#__codelineno-14-41"></a>        return LLTMFunction.apply(input, self.weights, self.bias, *state)
</code></pre></div>
<h4 id="_6">性能比较 <a href="#performance-comparison" title="永久链接到此标题">¶</a></h4>
<p>现在我们可以从 PyTorch 使用和调用 C++ 代码，我们可以运行一个小型基准测试来看看我们通过用 C++ 重写操作获得了多少性能。我们’ 将前后运行 LLTM 几次并测量
d持续时间：</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-15-1" name="__codelineno-15-1" href="#__codelineno-15-1"></a>import time
<a id="__codelineno-15-2" name="__codelineno-15-2" href="#__codelineno-15-2"></a>
<a id="__codelineno-15-3" name="__codelineno-15-3" href="#__codelineno-15-3"></a>import torch
<a id="__codelineno-15-4" name="__codelineno-15-4" href="#__codelineno-15-4"></a>
<a id="__codelineno-15-5" name="__codelineno-15-5" href="#__codelineno-15-5"></a>batch_size = 16
<a id="__codelineno-15-6" name="__codelineno-15-6" href="#__codelineno-15-6"></a>input_features = 32
<a id="__codelineno-15-7" name="__codelineno-15-7" href="#__codelineno-15-7"></a>state_size = 128
<a id="__codelineno-15-8" name="__codelineno-15-8" href="#__codelineno-15-8"></a>
<a id="__codelineno-15-9" name="__codelineno-15-9" href="#__codelineno-15-9"></a>X = torch.randn(batch_size, input_features)
<a id="__codelineno-15-10" name="__codelineno-15-10" href="#__codelineno-15-10"></a>h = torch.randn(batch_size, state_size)
<a id="__codelineno-15-11" name="__codelineno-15-11" href="#__codelineno-15-11"></a>C = torch.randn(batch_size, state_size)
<a id="__codelineno-15-12" name="__codelineno-15-12" href="#__codelineno-15-12"></a>
<a id="__codelineno-15-13" name="__codelineno-15-13" href="#__codelineno-15-13"></a>rnn = LLTM(input_features, state_size)
<a id="__codelineno-15-14" name="__codelineno-15-14" href="#__codelineno-15-14"></a>
<a id="__codelineno-15-15" name="__codelineno-15-15" href="#__codelineno-15-15"></a>forward = 0
<a id="__codelineno-15-16" name="__codelineno-15-16" href="#__codelineno-15-16"></a>backward = 0
<a id="__codelineno-15-17" name="__codelineno-15-17" href="#__codelineno-15-17"></a>for _ in range(100000):
<a id="__codelineno-15-18" name="__codelineno-15-18" href="#__codelineno-15-18"></a>    start = time.time()
<a id="__codelineno-15-19" name="__codelineno-15-19" href="#__codelineno-15-19"></a>    new_h, new_C = rnn(X, (h, C))
<a id="__codelineno-15-20" name="__codelineno-15-20" href="#__codelineno-15-20"></a>    forward += time.time() - start
<a id="__codelineno-15-21" name="__codelineno-15-21" href="#__codelineno-15-21"></a>
<a id="__codelineno-15-22" name="__codelineno-15-22" href="#__codelineno-15-22"></a>    start = time.time()
<a id="__codelineno-15-23" name="__codelineno-15-23" href="#__codelineno-15-23"></a>    (new_h.sum() + new_C.sum()).backward()
<a id="__codelineno-15-24" name="__codelineno-15-24" href="#__codelineno-15-24"></a>    backward += time.time() - start
<a id="__codelineno-15-25" name="__codelineno-15-25" href="#__codelineno-15-25"></a>
<a id="__codelineno-15-26" name="__codelineno-15-26" href="#__codelineno-15-26"></a>print(&#39;Forward: {:.3f} s | Backward {:.3f} s&#39;.format(forward, backward))
</code></pre></div>
<p>如果我们使用本文开头用纯 Python 编写的原始 LLTM 运行此代码
，我们会得到以下数字(在我的机器上)：</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-16-1" name="__codelineno-16-1" href="#__codelineno-16-1"></a>Forward: 506.480 us | Backward 444.694 us
</code></pre></div>
<p>以及我们新的 C++ 版本：</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-17-1" name="__codelineno-17-1" href="#__codelineno-17-1"></a>Forward: 349.335 us | Backward 443.523 us
</code></pre></div>
<p>我们已经可以看到前向函数的显着加速(超过
30%)。对于后向函数，加速是可见的，尽管不是主要的。
我上面写的后向传递没有特别优化，
肯定可以改进。此外，PyTorch’s 自动微分引擎
可以自动并行化计算图，
可以使用整体上更高效的
操作流程，并且也是用 C++ 实现的，因此’s
预计会很快
 。尽管如此，这是一个好的开始。</p>
<h4 id="gpu">GPU 设备上的性能 <a href="#performance-on-gpu-devices" title="此标题的永久链接">¶</a></h4>
<p>关于 PyTorch’s
 <em>ATen</em> 
 后端的一个奇妙事实是，它抽象
您正在运行的计算设备。这意味着我们为 CPU 编写的相同代码也可以在 GPU 上运行，并且各个操作将相应地分派给 GPU 优化的实现。对于诸如矩阵乘法之类的某些操作(如 
 <code>mm</code>
 或 
 <code>addmm</code>
 )，这是一个巨大的胜利。让’s 看看我们通过使用 CUDA tensor运行 C++ 代码获得了多少性能。不需要对我们的实现进行任何更改，我们只需将tensor从 Python 放入 GPU 内存中，在创建时添加
 <code>device=cuda_device</code>
 参数或使用
 <code>.to (cuda_device)</code>
 创建后：</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-18-1" name="__codelineno-18-1" href="#__codelineno-18-1"></a>import torch
<a id="__codelineno-18-2" name="__codelineno-18-2" href="#__codelineno-18-2"></a>
<a id="__codelineno-18-3" name="__codelineno-18-3" href="#__codelineno-18-3"></a>assert torch.cuda.is_available()
<a id="__codelineno-18-4" name="__codelineno-18-4" href="#__codelineno-18-4"></a>cuda_device = torch.device(&quot;cuda&quot;)  # device object representing GPU
<a id="__codelineno-18-5" name="__codelineno-18-5" href="#__codelineno-18-5"></a>
<a id="__codelineno-18-6" name="__codelineno-18-6" href="#__codelineno-18-6"></a>batch_size = 16
<a id="__codelineno-18-7" name="__codelineno-18-7" href="#__codelineno-18-7"></a>input_features = 32
<a id="__codelineno-18-8" name="__codelineno-18-8" href="#__codelineno-18-8"></a>state_size = 128
<a id="__codelineno-18-9" name="__codelineno-18-9" href="#__codelineno-18-9"></a>
<a id="__codelineno-18-10" name="__codelineno-18-10" href="#__codelineno-18-10"></a># Note the device=cuda_device arguments here
<a id="__codelineno-18-11" name="__codelineno-18-11" href="#__codelineno-18-11"></a>X = torch.randn(batch_size, input_features, device=cuda_device)
<a id="__codelineno-18-12" name="__codelineno-18-12" href="#__codelineno-18-12"></a>h = torch.randn(batch_size, state_size, device=cuda_device)
<a id="__codelineno-18-13" name="__codelineno-18-13" href="#__codelineno-18-13"></a>C = torch.randn(batch_size, state_size, device=cuda_device)
<a id="__codelineno-18-14" name="__codelineno-18-14" href="#__codelineno-18-14"></a>
<a id="__codelineno-18-15" name="__codelineno-18-15" href="#__codelineno-18-15"></a>rnn = LLTM(input_features, state_size).to(cuda_device)
<a id="__codelineno-18-16" name="__codelineno-18-16" href="#__codelineno-18-16"></a>
<a id="__codelineno-18-17" name="__codelineno-18-17" href="#__codelineno-18-17"></a>forward = 0
<a id="__codelineno-18-18" name="__codelineno-18-18" href="#__codelineno-18-18"></a>backward = 0
<a id="__codelineno-18-19" name="__codelineno-18-19" href="#__codelineno-18-19"></a>for _ in range(100000):
<a id="__codelineno-18-20" name="__codelineno-18-20" href="#__codelineno-18-20"></a>    start = time.time()
<a id="__codelineno-18-21" name="__codelineno-18-21" href="#__codelineno-18-21"></a>    new_h, new_C = rnn(X, (h, C))
<a id="__codelineno-18-22" name="__codelineno-18-22" href="#__codelineno-18-22"></a>    torch.cuda.synchronize()
<a id="__codelineno-18-23" name="__codelineno-18-23" href="#__codelineno-18-23"></a>    forward += time.time() - start
<a id="__codelineno-18-24" name="__codelineno-18-24" href="#__codelineno-18-24"></a>
<a id="__codelineno-18-25" name="__codelineno-18-25" href="#__codelineno-18-25"></a>    start = time.time()
<a id="__codelineno-18-26" name="__codelineno-18-26" href="#__codelineno-18-26"></a>    (new_h.sum() + new_C.sum()).backward()
<a id="__codelineno-18-27" name="__codelineno-18-27" href="#__codelineno-18-27"></a>    torch.cuda.synchronize()
<a id="__codelineno-18-28" name="__codelineno-18-28" href="#__codelineno-18-28"></a>    backward += time.time() - start
<a id="__codelineno-18-29" name="__codelineno-18-29" href="#__codelineno-18-29"></a>
<a id="__codelineno-18-30" name="__codelineno-18-30" href="#__codelineno-18-30"></a>print(&#39;Forward: {:.3f} us | Backward {:.3f} us&#39;.format(forward * 1e6/1e5, backward * 1e6/1e5))
</code></pre></div>
<p>再次将我们的普通 PyTorch 代码与 C++ 版本进行比较，现在两者都在 CUDA 设备上运行，我们再次看到性能提升。对于 Python/PyTorch：</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-19-1" name="__codelineno-19-1" href="#__codelineno-19-1"></a>Forward: 187.719 us | Backward 410.815 us
</code></pre></div>
<p>和 C++/ATen:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-20-1" name="__codelineno-20-1" href="#__codelineno-20-1"></a>Forward: 149.802 us | Backward 393.458 us
</code></pre></div>
<p>与非 CUDA 代码相比，’ 的整体加速效果非常好。但是，我们可以通过编写自定义 CUDA 内核来提高 C++ 代码的性能，我们很快就会深入研究该内核。在此之前，让’s 讨论一下构建 C++
扩展的另一种方法。</p>
<h3 id="jit">JIT 编译扩展 <a href="#jit-compiling-extensions" title="永久链接到此标题">¶</a></h3>
<p>之前，我提到有两种构建 C++ 扩展的方法：使用
 <code>setuptools</code>
 或即时 (JIT)。介绍完前者后，让’s
e 详细讨论后者。 JIT 编译机制为您提供了一种通过调用 PyTorch’s API 中名为
 <a href="https://pytorch.org/docs/stable/cpp_extension.html#torch.utils.cpp_extension.load" title="(在 PyTorch v2.1 中)"><code>torch.utils.cpp_extension.load( )</code></a>
 。对于
LLTM，这看起来就像这样简单：</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-21-1" name="__codelineno-21-1" href="#__codelineno-21-1"></a>from torch.utils.cpp_extension import load
<a id="__codelineno-21-2" name="__codelineno-21-2" href="#__codelineno-21-2"></a>
<a id="__codelineno-21-3" name="__codelineno-21-3" href="#__codelineno-21-3"></a>lltm_cpp = load(name=&quot;lltm_cpp&quot;, sources=[&quot;lltm.cpp&quot;])
</code></pre></div>
<p>在这里，我们为函数提供与 <code>setuptools</code> 相同的信息。在后台，这将执行以下操作:</p>
<ol>
<li>创建临时目录
 <code>/tmp/torch_extensions/lltm</code>
 ,
2.将
 <a href="https://ninja-build.org/">Ninja</a> 
 构建文件发送到该临时目录中，
3.将源文件编译到共享库中，
4.将此共享库导入为 Python 模块。</li>
</ol>
<p>事实上，如果您将
 <code>verbose=True</code>
 传递给
 <code>cpp_extension.load()</code>
 ，您将
被告知该过程：</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-22-1" name="__codelineno-22-1" href="#__codelineno-22-1"></a>Using /tmp/torch_extensions as PyTorch extensions root...
<a id="__codelineno-22-2" name="__codelineno-22-2" href="#__codelineno-22-2"></a>Emitting ninja build file /tmp/torch_extensions/lltm_cpp/build.ninja...
<a id="__codelineno-22-3" name="__codelineno-22-3" href="#__codelineno-22-3"></a>Building extension module lltm_cpp...
<a id="__codelineno-22-4" name="__codelineno-22-4" href="#__codelineno-22-4"></a>Loading extension module lltm_cpp...
</code></pre></div>
<p>生成的 Python 模块将与 setuptools 生成的完全相同，
但消除了必须维护单独
 <code>setup.py</code>
 build
文件的要求。如果您的设置更复杂并且您确实需要 
 <code>setuptools</code>
 的全部功能，您
 <em>可以</em> 
 编写自己的
 <code>setup.py</code>
 \xe2\x80\x93 但在很多情况下，这种 JIT 技术就可以很好地发挥作用。第一次运行此行时，
it 将需要一些时间，因为扩展正在后台编译。由于
我们使用 Ninja 构建系统来构建源代码，因此重新编译是增量的，因此，如果您没有\xe2\x80\，则在第二次运行 Python 模块时
重新加载扩展会很快且开销较低x99t 更改扩展名\xe2\x80\x99s
源文件。</p>
<h2 id="ccuda">编写混合 C++/CUDA 扩展 <a href="#writing-a-mixed-c-cuda-extension" title="永久链接到此标题">¶</a></h2>
<p>为了真正将我们的实现提升到一个新的水平，我们可以使用自定义 CUDA 内核手写部分前向和后向传递。对于 LLTM，这有望特别有效，因为存在大量按顺序进行的逐点操作，这些操作都可以在单个 CUDA 内核中融合和并行化。让’s 看看我们如何编写这样的 CUDA 内核并
使用此扩展机制将其与 PyTorch 集成。</p>
<p>编写 CUDA 扩展的一般策略是首先编写一个 C++ 文件，该文件定义将从 Python 调用的函数，并使用 pybind11 将这些函数绑定到 Python。此外，此文件还将声明 CUDA (
 <code>.cu</code>
 ) 文件中定义的函数。然后，C++ 函数将执行一些检查并最终将其调用转发给 CUDA 函数。在 CUDA 文件中，我们编写实际的 CUDA 内核。然后，
 <code>cpp_extension</code>
 包
将使用 C++ 编译器(如
 <code>gcc</code>
)来编译 C++ 源代码，并使用 NVIDIA’s
 <code>nvcc 来编译 CUDA 源代码</code>
 编译器。这可确保每个编译器处理它最了解的要编译的文件。最终，它们
将链接到一个共享库，我们可以通过 Python
代码使用该库。</p>
<p>我们’ll 从 C++ 文件开始，我们’ll 将其称为
 <code>lltm_cuda.cpp</code>
 ，例如：</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-23-1" name="__codelineno-23-1" href="#__codelineno-23-1"></a>#include &lt;torch/extension.h&gt;
<a id="__codelineno-23-2" name="__codelineno-23-2" href="#__codelineno-23-2"></a>
<a id="__codelineno-23-3" name="__codelineno-23-3" href="#__codelineno-23-3"></a>#include &lt;vector&gt;
<a id="__codelineno-23-4" name="__codelineno-23-4" href="#__codelineno-23-4"></a>
<a id="__codelineno-23-5" name="__codelineno-23-5" href="#__codelineno-23-5"></a>// CUDA forward declarations
<a id="__codelineno-23-6" name="__codelineno-23-6" href="#__codelineno-23-6"></a>
<a id="__codelineno-23-7" name="__codelineno-23-7" href="#__codelineno-23-7"></a>std::vector&lt;torch::Tensor&gt; lltm_cuda_forward(
<a id="__codelineno-23-8" name="__codelineno-23-8" href="#__codelineno-23-8"></a> torch::Tensor input,
<a id="__codelineno-23-9" name="__codelineno-23-9" href="#__codelineno-23-9"></a> torch::Tensor weights,
<a id="__codelineno-23-10" name="__codelineno-23-10" href="#__codelineno-23-10"></a> torch::Tensor bias,
<a id="__codelineno-23-11" name="__codelineno-23-11" href="#__codelineno-23-11"></a> torch::Tensor old_h,
<a id="__codelineno-23-12" name="__codelineno-23-12" href="#__codelineno-23-12"></a> torch::Tensor old_cell);
<a id="__codelineno-23-13" name="__codelineno-23-13" href="#__codelineno-23-13"></a>
<a id="__codelineno-23-14" name="__codelineno-23-14" href="#__codelineno-23-14"></a>std::vector&lt;torch::Tensor&gt; lltm_cuda_backward(
<a id="__codelineno-23-15" name="__codelineno-23-15" href="#__codelineno-23-15"></a> torch::Tensor grad_h,
<a id="__codelineno-23-16" name="__codelineno-23-16" href="#__codelineno-23-16"></a> torch::Tensor grad_cell,
<a id="__codelineno-23-17" name="__codelineno-23-17" href="#__codelineno-23-17"></a> torch::Tensor new_cell,
<a id="__codelineno-23-18" name="__codelineno-23-18" href="#__codelineno-23-18"></a> torch::Tensor input_gate,
<a id="__codelineno-23-19" name="__codelineno-23-19" href="#__codelineno-23-19"></a> torch::Tensor output_gate,
<a id="__codelineno-23-20" name="__codelineno-23-20" href="#__codelineno-23-20"></a> torch::Tensor candidate_cell,
<a id="__codelineno-23-21" name="__codelineno-23-21" href="#__codelineno-23-21"></a> torch::Tensor X,
<a id="__codelineno-23-22" name="__codelineno-23-22" href="#__codelineno-23-22"></a> torch::Tensor gate_weights,
<a id="__codelineno-23-23" name="__codelineno-23-23" href="#__codelineno-23-23"></a> torch::Tensor weights);
<a id="__codelineno-23-24" name="__codelineno-23-24" href="#__codelineno-23-24"></a>
<a id="__codelineno-23-25" name="__codelineno-23-25" href="#__codelineno-23-25"></a>// C++ interface
<a id="__codelineno-23-26" name="__codelineno-23-26" href="#__codelineno-23-26"></a>
<a id="__codelineno-23-27" name="__codelineno-23-27" href="#__codelineno-23-27"></a>#define CHECK_CUDA(x) TORCH_CHECK(x.device().is_cuda(), #x &quot; must be a CUDA tensor&quot;)
<a id="__codelineno-23-28" name="__codelineno-23-28" href="#__codelineno-23-28"></a>#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x &quot; must be contiguous&quot;)
<a id="__codelineno-23-29" name="__codelineno-23-29" href="#__codelineno-23-29"></a>#define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)
<a id="__codelineno-23-30" name="__codelineno-23-30" href="#__codelineno-23-30"></a>
<a id="__codelineno-23-31" name="__codelineno-23-31" href="#__codelineno-23-31"></a>std::vector&lt;torch::Tensor&gt; lltm_forward(
<a id="__codelineno-23-32" name="__codelineno-23-32" href="#__codelineno-23-32"></a> torch::Tensor input,
<a id="__codelineno-23-33" name="__codelineno-23-33" href="#__codelineno-23-33"></a> torch::Tensor weights,
<a id="__codelineno-23-34" name="__codelineno-23-34" href="#__codelineno-23-34"></a> torch::Tensor bias,
<a id="__codelineno-23-35" name="__codelineno-23-35" href="#__codelineno-23-35"></a> torch::Tensor old_h,
<a id="__codelineno-23-36" name="__codelineno-23-36" href="#__codelineno-23-36"></a> torch::Tensor old_cell) {
<a id="__codelineno-23-37" name="__codelineno-23-37" href="#__codelineno-23-37"></a> CHECK_INPUT(input);
<a id="__codelineno-23-38" name="__codelineno-23-38" href="#__codelineno-23-38"></a> CHECK_INPUT(weights);
<a id="__codelineno-23-39" name="__codelineno-23-39" href="#__codelineno-23-39"></a> CHECK_INPUT(bias);
<a id="__codelineno-23-40" name="__codelineno-23-40" href="#__codelineno-23-40"></a> CHECK_INPUT(old_h);
<a id="__codelineno-23-41" name="__codelineno-23-41" href="#__codelineno-23-41"></a> CHECK_INPUT(old_cell);
<a id="__codelineno-23-42" name="__codelineno-23-42" href="#__codelineno-23-42"></a>
<a id="__codelineno-23-43" name="__codelineno-23-43" href="#__codelineno-23-43"></a> return lltm_cuda_forward(input, weights, bias, old_h, old_cell);
<a id="__codelineno-23-44" name="__codelineno-23-44" href="#__codelineno-23-44"></a>}
<a id="__codelineno-23-45" name="__codelineno-23-45" href="#__codelineno-23-45"></a>
<a id="__codelineno-23-46" name="__codelineno-23-46" href="#__codelineno-23-46"></a>std::vector&lt;torch::Tensor&gt; lltm_backward(
<a id="__codelineno-23-47" name="__codelineno-23-47" href="#__codelineno-23-47"></a> torch::Tensor grad_h,
<a id="__codelineno-23-48" name="__codelineno-23-48" href="#__codelineno-23-48"></a> torch::Tensor grad_cell,
<a id="__codelineno-23-49" name="__codelineno-23-49" href="#__codelineno-23-49"></a> torch::Tensor new_cell,
<a id="__codelineno-23-50" name="__codelineno-23-50" href="#__codelineno-23-50"></a> torch::Tensor input_gate,
<a id="__codelineno-23-51" name="__codelineno-23-51" href="#__codelineno-23-51"></a> torch::Tensor output_gate,
<a id="__codelineno-23-52" name="__codelineno-23-52" href="#__codelineno-23-52"></a> torch::Tensor candidate_cell,
<a id="__codelineno-23-53" name="__codelineno-23-53" href="#__codelineno-23-53"></a> torch::Tensor X,
<a id="__codelineno-23-54" name="__codelineno-23-54" href="#__codelineno-23-54"></a> torch::Tensor gate_weights,
<a id="__codelineno-23-55" name="__codelineno-23-55" href="#__codelineno-23-55"></a> torch::Tensor weights) {
<a id="__codelineno-23-56" name="__codelineno-23-56" href="#__codelineno-23-56"></a> CHECK_INPUT(grad_h);
<a id="__codelineno-23-57" name="__codelineno-23-57" href="#__codelineno-23-57"></a> CHECK_INPUT(grad_cell);
<a id="__codelineno-23-58" name="__codelineno-23-58" href="#__codelineno-23-58"></a> CHECK_INPUT(input_gate);
<a id="__codelineno-23-59" name="__codelineno-23-59" href="#__codelineno-23-59"></a> CHECK_INPUT(output_gate);
<a id="__codelineno-23-60" name="__codelineno-23-60" href="#__codelineno-23-60"></a> CHECK_INPUT(candidate_cell);
<a id="__codelineno-23-61" name="__codelineno-23-61" href="#__codelineno-23-61"></a> CHECK_INPUT(X);
<a id="__codelineno-23-62" name="__codelineno-23-62" href="#__codelineno-23-62"></a> CHECK_INPUT(gate_weights);
<a id="__codelineno-23-63" name="__codelineno-23-63" href="#__codelineno-23-63"></a> CHECK_INPUT(weights);
<a id="__codelineno-23-64" name="__codelineno-23-64" href="#__codelineno-23-64"></a>
<a id="__codelineno-23-65" name="__codelineno-23-65" href="#__codelineno-23-65"></a> return lltm_cuda_backward(
<a id="__codelineno-23-66" name="__codelineno-23-66" href="#__codelineno-23-66"></a> grad_h,
<a id="__codelineno-23-67" name="__codelineno-23-67" href="#__codelineno-23-67"></a> grad_cell,
<a id="__codelineno-23-68" name="__codelineno-23-68" href="#__codelineno-23-68"></a> new_cell,
<a id="__codelineno-23-69" name="__codelineno-23-69" href="#__codelineno-23-69"></a> input_gate,
<a id="__codelineno-23-70" name="__codelineno-23-70" href="#__codelineno-23-70"></a> output_gate,
<a id="__codelineno-23-71" name="__codelineno-23-71" href="#__codelineno-23-71"></a> candidate_cell,
<a id="__codelineno-23-72" name="__codelineno-23-72" href="#__codelineno-23-72"></a> X,
<a id="__codelineno-23-73" name="__codelineno-23-73" href="#__codelineno-23-73"></a> gate_weights,
<a id="__codelineno-23-74" name="__codelineno-23-74" href="#__codelineno-23-74"></a> weights);
<a id="__codelineno-23-75" name="__codelineno-23-75" href="#__codelineno-23-75"></a>}
<a id="__codelineno-23-76" name="__codelineno-23-76" href="#__codelineno-23-76"></a>
<a id="__codelineno-23-77" name="__codelineno-23-77" href="#__codelineno-23-77"></a>PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
<a id="__codelineno-23-78" name="__codelineno-23-78" href="#__codelineno-23-78"></a> m.def(&quot;forward&quot;, &amp;lltm_forward, &quot;LLTM forward (CUDA)&quot;);
<a id="__codelineno-23-79" name="__codelineno-23-79" href="#__codelineno-23-79"></a> m.def(&quot;backward&quot;, &amp;lltm_backward, &quot;LLTM backward (CUDA)&quot;);
<a id="__codelineno-23-80" name="__codelineno-23-80" href="#__codelineno-23-80"></a>}
</code></pre></div>
<p>如您所见，它主要是样板文件，检查并转发到我们’ 将在 CUDA 文件中定义的函数。我们’ 将此文件命名为
 <code>lltm_cuda_kernel.cu</code>
(注意
 <code>.cu</code>
 扩展名！)。 NVCC 可以合理
编译 C++11，因此我们仍然可以使用 ATen 和 C++ 标准库(但不是
 <code>torch.h</code>
 )。请注意，
 <code>setuptools</code>
 无法处理
具有相同名称但扩展名不同的文件，因此，如果您使用
 <code>setup.py</code>
 方法而不是 JIT 方法，则必须为您的 CUDA 文件指定不同的名称
而不是 C++ 文件的名称(对于 JIT 方法，
 <code>lltm.cpp</code>
 和
 <code>lltm.cu</code>
 可以正常工作)。让’s 看一下这个文件的样子：</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-24-1" name="__codelineno-24-1" href="#__codelineno-24-1"></a>#include &lt;torch/extension.h&gt;
<a id="__codelineno-24-2" name="__codelineno-24-2" href="#__codelineno-24-2"></a>
<a id="__codelineno-24-3" name="__codelineno-24-3" href="#__codelineno-24-3"></a>#include &lt;cuda.h&gt;
<a id="__codelineno-24-4" name="__codelineno-24-4" href="#__codelineno-24-4"></a>#include &lt;cuda_runtime.h&gt;
<a id="__codelineno-24-5" name="__codelineno-24-5" href="#__codelineno-24-5"></a>
<a id="__codelineno-24-6" name="__codelineno-24-6" href="#__codelineno-24-6"></a>#include &lt;vector&gt;
<a id="__codelineno-24-7" name="__codelineno-24-7" href="#__codelineno-24-7"></a>
<a id="__codelineno-24-8" name="__codelineno-24-8" href="#__codelineno-24-8"></a>template &lt;typename scalar_t&gt;
<a id="__codelineno-24-9" name="__codelineno-24-9" href="#__codelineno-24-9"></a>__device__ __forceinline__ scalar_t sigmoid(scalar_t z) {
<a id="__codelineno-24-10" name="__codelineno-24-10" href="#__codelineno-24-10"></a> return 1.0 / (1.0 + exp(-z));
<a id="__codelineno-24-11" name="__codelineno-24-11" href="#__codelineno-24-11"></a>}
</code></pre></div>
<p>在这里我们看到我刚刚描述的标头，以及我们正在使用
CUDA 特定声明的事实，例如 
 <code>__device__</code>
 和 
 <code>__forceinline__</code>
 和
类似
 <code>exp</code>
 的函数。让’s 继续使用
我们’ 需要的更多辅助函数：</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-25-1" name="__codelineno-25-1" href="#__codelineno-25-1"></a>template &lt;typename scalar_t&gt;
<a id="__codelineno-25-2" name="__codelineno-25-2" href="#__codelineno-25-2"></a>__device__ __forceinline__ scalar_t d_sigmoid(scalar_t z) {
<a id="__codelineno-25-3" name="__codelineno-25-3" href="#__codelineno-25-3"></a> const auto s = sigmoid(z);
<a id="__codelineno-25-4" name="__codelineno-25-4" href="#__codelineno-25-4"></a> return (1.0 - s) * s;
<a id="__codelineno-25-5" name="__codelineno-25-5" href="#__codelineno-25-5"></a>}
<a id="__codelineno-25-6" name="__codelineno-25-6" href="#__codelineno-25-6"></a>
<a id="__codelineno-25-7" name="__codelineno-25-7" href="#__codelineno-25-7"></a>template &lt;typename scalar_t&gt;
<a id="__codelineno-25-8" name="__codelineno-25-8" href="#__codelineno-25-8"></a>__device__ __forceinline__ scalar_t d_tanh(scalar_t z) {
<a id="__codelineno-25-9" name="__codelineno-25-9" href="#__codelineno-25-9"></a> const auto t = tanh(z);
<a id="__codelineno-25-10" name="__codelineno-25-10" href="#__codelineno-25-10"></a> return 1 - (t * t);
<a id="__codelineno-25-11" name="__codelineno-25-11" href="#__codelineno-25-11"></a>}
<a id="__codelineno-25-12" name="__codelineno-25-12" href="#__codelineno-25-12"></a>
<a id="__codelineno-25-13" name="__codelineno-25-13" href="#__codelineno-25-13"></a>template &lt;typename scalar_t&gt;
<a id="__codelineno-25-14" name="__codelineno-25-14" href="#__codelineno-25-14"></a>__device__ __forceinline__ scalar_t elu(scalar_t z, scalar_t alpha = 1.0) {
<a id="__codelineno-25-15" name="__codelineno-25-15" href="#__codelineno-25-15"></a> return fmax(0.0, z) + fmin(0.0, alpha * (exp(z) - 1.0));
<a id="__codelineno-25-16" name="__codelineno-25-16" href="#__codelineno-25-16"></a>}
<a id="__codelineno-25-17" name="__codelineno-25-17" href="#__codelineno-25-17"></a>
<a id="__codelineno-25-18" name="__codelineno-25-18" href="#__codelineno-25-18"></a>template &lt;typename scalar_t&gt;
<a id="__codelineno-25-19" name="__codelineno-25-19" href="#__codelineno-25-19"></a>__device__ __forceinline__ scalar_t d_elu(scalar_t z, scalar_t alpha = 1.0) {
<a id="__codelineno-25-20" name="__codelineno-25-20" href="#__codelineno-25-20"></a> const auto e = exp(z);
<a id="__codelineno-25-21" name="__codelineno-25-21" href="#__codelineno-25-21"></a> const auto d_relu = z &lt; 0.0 ? 0.0 : 1.0;
<a id="__codelineno-25-22" name="__codelineno-25-22" href="#__codelineno-25-22"></a> return d_relu + (((alpha * (e - 1.0)) &lt; 0.0) ? (alpha * e) : 0.0);
<a id="__codelineno-25-23" name="__codelineno-25-23" href="#__codelineno-25-23"></a>}
</code></pre></div>
<p>现在要实际实现一个函数，我们’ 将再次需要两件事：一个函数
执行我们不’ 不希望显式手动编写的操作并调用
到 CUDA 内核，然后是我们想要加速的部分的实际 CUDA 内核。对于前向传播，第一个函数应如下所示：</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-26-1" name="__codelineno-26-1" href="#__codelineno-26-1"></a>std::vector&lt;torch::Tensor&gt; lltm_cuda_forward(
<a id="__codelineno-26-2" name="__codelineno-26-2" href="#__codelineno-26-2"></a> torch::Tensor input,
<a id="__codelineno-26-3" name="__codelineno-26-3" href="#__codelineno-26-3"></a> torch::Tensor weights,
<a id="__codelineno-26-4" name="__codelineno-26-4" href="#__codelineno-26-4"></a> torch::Tensor bias,
<a id="__codelineno-26-5" name="__codelineno-26-5" href="#__codelineno-26-5"></a> torch::Tensor old_h,
<a id="__codelineno-26-6" name="__codelineno-26-6" href="#__codelineno-26-6"></a> torch::Tensor old_cell) {
<a id="__codelineno-26-7" name="__codelineno-26-7" href="#__codelineno-26-7"></a> auto X = torch::cat({old_h, input}, /*dim=*/1);
<a id="__codelineno-26-8" name="__codelineno-26-8" href="#__codelineno-26-8"></a> auto gates = torch::addmm(bias, X, weights.transpose(0, 1));
<a id="__codelineno-26-9" name="__codelineno-26-9" href="#__codelineno-26-9"></a>
<a id="__codelineno-26-10" name="__codelineno-26-10" href="#__codelineno-26-10"></a> const auto batch_size = old_cell.size(0);
<a id="__codelineno-26-11" name="__codelineno-26-11" href="#__codelineno-26-11"></a> const auto state_size = old_cell.size(1);
<a id="__codelineno-26-12" name="__codelineno-26-12" href="#__codelineno-26-12"></a>
<a id="__codelineno-26-13" name="__codelineno-26-13" href="#__codelineno-26-13"></a> auto new_h = torch::zeros_like(old_cell);
<a id="__codelineno-26-14" name="__codelineno-26-14" href="#__codelineno-26-14"></a> auto new_cell = torch::zeros_like(old_cell);
<a id="__codelineno-26-15" name="__codelineno-26-15" href="#__codelineno-26-15"></a> auto input_gate = torch::zeros_like(old_cell);
<a id="__codelineno-26-16" name="__codelineno-26-16" href="#__codelineno-26-16"></a> auto output_gate = torch::zeros_like(old_cell);
<a id="__codelineno-26-17" name="__codelineno-26-17" href="#__codelineno-26-17"></a> auto candidate_cell = torch::zeros_like(old_cell);
<a id="__codelineno-26-18" name="__codelineno-26-18" href="#__codelineno-26-18"></a>
<a id="__codelineno-26-19" name="__codelineno-26-19" href="#__codelineno-26-19"></a> const int threads = 1024;
<a id="__codelineno-26-20" name="__codelineno-26-20" href="#__codelineno-26-20"></a> const dim3 blocks((state_size + threads - 1) / threads, batch_size);
<a id="__codelineno-26-21" name="__codelineno-26-21" href="#__codelineno-26-21"></a>
<a id="__codelineno-26-22" name="__codelineno-26-22" href="#__codelineno-26-22"></a> AT_DISPATCH_FLOATING_TYPES(gates.type(), &quot;lltm_forward_cuda&quot;, ([&amp;] {
<a id="__codelineno-26-23" name="__codelineno-26-23" href="#__codelineno-26-23"></a> lltm_cuda_forward_kernel&lt;scalar_t&gt;&lt;&lt;&lt;blocks, threads&gt;&gt;&gt;(
<a id="__codelineno-26-24" name="__codelineno-26-24" href="#__codelineno-26-24"></a> gates.data&lt;scalar_t&gt;(),
<a id="__codelineno-26-25" name="__codelineno-26-25" href="#__codelineno-26-25"></a> old_cell.data&lt;scalar_t&gt;(),
<a id="__codelineno-26-26" name="__codelineno-26-26" href="#__codelineno-26-26"></a> new_h.data&lt;scalar_t&gt;(),
<a id="__codelineno-26-27" name="__codelineno-26-27" href="#__codelineno-26-27"></a> new_cell.data&lt;scalar_t&gt;(),
<a id="__codelineno-26-28" name="__codelineno-26-28" href="#__codelineno-26-28"></a> input_gate.data&lt;scalar_t&gt;(),
<a id="__codelineno-26-29" name="__codelineno-26-29" href="#__codelineno-26-29"></a> output_gate.data&lt;scalar_t&gt;(),
<a id="__codelineno-26-30" name="__codelineno-26-30" href="#__codelineno-26-30"></a> candidate_cell.data&lt;scalar_t&gt;(),
<a id="__codelineno-26-31" name="__codelineno-26-31" href="#__codelineno-26-31"></a> state_size);
<a id="__codelineno-26-32" name="__codelineno-26-32" href="#__codelineno-26-32"></a> }));
<a id="__codelineno-26-33" name="__codelineno-26-33" href="#__codelineno-26-33"></a>
<a id="__codelineno-26-34" name="__codelineno-26-34" href="#__codelineno-26-34"></a> return {new_h, new_cell, input_gate, output_gate, candidate_cell, X, gates};
<a id="__codelineno-26-35" name="__codelineno-26-35" href="#__codelineno-26-35"></a>}
</code></pre></div>
<p>这里的主要兴趣点是
 <code>AT_DISPATCH_FLOATING_TYPES</code>
 宏和
内核启动(由
 <code>&lt;&lt;&lt;...&gt;&gt;&gt; 表示)</code>
)。虽然 ATen 抽象出了我们处理的tensor的设备和数据类型，但tensor在运行时仍由具体设备上的具体类型的内存支持。因此，我们需要一种在运行时确定tensor是什么类型的方法，然后有选择地调用具有相应正确类型签名的函数。手动完成，
这(概念上)看起来像这样：</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-27-1" name="__codelineno-27-1" href="#__codelineno-27-1"></a>switch (tensor.type().scalarType()) {
<a id="__codelineno-27-2" name="__codelineno-27-2" href="#__codelineno-27-2"></a> case torch::ScalarType::Double:
<a id="__codelineno-27-3" name="__codelineno-27-3" href="#__codelineno-27-3"></a> return function&lt;double&gt;(tensor.data&lt;double&gt;());
<a id="__codelineno-27-4" name="__codelineno-27-4" href="#__codelineno-27-4"></a> case torch::ScalarType::Float:
<a id="__codelineno-27-5" name="__codelineno-27-5" href="#__codelineno-27-5"></a> return function&lt;float&gt;(tensor.data&lt;float&gt;());
<a id="__codelineno-27-6" name="__codelineno-27-6" href="#__codelineno-27-6"></a> ...
<a id="__codelineno-27-7" name="__codelineno-27-7" href="#__codelineno-27-7"></a>}
</code></pre></div>
<p><code>AT_DISPATCH_FLOATING_TYPES</code>
 的目的是为我们处理这个调度。它需要一个类型(
 <code>gates.type()</code>
 在我们的例子中)、一个名称(用于错误消息)和一个 lambda 函数。在此 lambda 函数内，类型别名
 <code>scalar_t</code>
 可用，并被定义为tensor在运行时在该上下文中实际存在的类型。因此，如果我们有一个模板函数(我们的 CUDA 内核就是这个函数)，我们可以使用这个“scalar_t”别名来实例化它，并调用正确的函数。在这种情况下，我们还想检索tensor的数据指针作为该“标量_t”类型的指针。如果您想要分派所有类型而不仅仅是浮点类型(
 <code>Float</code>
 和 
 <code>Double</code>
 )，您可以使用
 <code>AT_DISPATCH_ALL_TYPES</code> 
.</p>
<p>请注意，我们使用普通 ATen 执行一些操作。这些操作仍将在 GPU 上运行，但使用 ATen’s 默认实现。这是有意义的，因为 ATen 将使用高度优化的例程来处理诸如矩阵乘法(例如，<code>addmm</code>)或卷积之类的事情，而这将更难以
实现和改进我们自己。</p>
<p>至于内核启动本身，我们在这里指定每个 CUDA 块
将有 1024 个线程，并且整个 GPU 网格被分成
多个块
 `1</p>
<p>x
 \每个组件需要 n
 1024<code>个线程来填充我们的矩阵。例如，如果我们的状态大小为 2048，批次大小为 4，则我们’d 总共启动</code>4</p>
<p>x</p>
<p>2</p>
<p>=</p>
<p>8`
 块，每块 1024 个线程。如果
您’之前从未听说过 CUDA “blocks” 或 “grids”，
 [关于 CUDA 的介绍性阅读
] (https://devblogs.nvidia.com/even-easier-introduction-cuda)
 可能
有帮助。</p>
<p>实际的 CUDA 内核相当简单(如果您’ 以前曾经编写过 GPU)：</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-28-1" name="__codelineno-28-1" href="#__codelineno-28-1"></a>template &lt;typename scalar_t&gt;
<a id="__codelineno-28-2" name="__codelineno-28-2" href="#__codelineno-28-2"></a>__global__ void lltm_cuda_forward_kernel(
<a id="__codelineno-28-3" name="__codelineno-28-3" href="#__codelineno-28-3"></a> const scalar_t* __restrict__ gates,
<a id="__codelineno-28-4" name="__codelineno-28-4" href="#__codelineno-28-4"></a> const scalar_t* __restrict__ old_cell,
<a id="__codelineno-28-5" name="__codelineno-28-5" href="#__codelineno-28-5"></a> scalar_t* __restrict__ new_h,
<a id="__codelineno-28-6" name="__codelineno-28-6" href="#__codelineno-28-6"></a> scalar_t* __restrict__ new_cell,
<a id="__codelineno-28-7" name="__codelineno-28-7" href="#__codelineno-28-7"></a> scalar_t* __restrict__ input_gate,
<a id="__codelineno-28-8" name="__codelineno-28-8" href="#__codelineno-28-8"></a> scalar_t* __restrict__ output_gate,
<a id="__codelineno-28-9" name="__codelineno-28-9" href="#__codelineno-28-9"></a> scalar_t* __restrict__ candidate_cell,
<a id="__codelineno-28-10" name="__codelineno-28-10" href="#__codelineno-28-10"></a> size_t state_size) {
<a id="__codelineno-28-11" name="__codelineno-28-11" href="#__codelineno-28-11"></a> const int column = blockIdx.x * blockDim.x + threadIdx.x;
<a id="__codelineno-28-12" name="__codelineno-28-12" href="#__codelineno-28-12"></a> const int index = blockIdx.y * state_size + column;
<a id="__codelineno-28-13" name="__codelineno-28-13" href="#__codelineno-28-13"></a> const int gates_row = blockIdx.y * (state_size * 3);
<a id="__codelineno-28-14" name="__codelineno-28-14" href="#__codelineno-28-14"></a> if (column &lt; state_size) {
<a id="__codelineno-28-15" name="__codelineno-28-15" href="#__codelineno-28-15"></a> input_gate[index] = sigmoid(gates[gates_row + column]);
<a id="__codelineno-28-16" name="__codelineno-28-16" href="#__codelineno-28-16"></a> output_gate[index] = sigmoid(gates[gates_row + state_size + column]);
<a id="__codelineno-28-17" name="__codelineno-28-17" href="#__codelineno-28-17"></a> candidate_cell[index] = elu(gates[gates_row + 2 * state_size + column]);
<a id="__codelineno-28-18" name="__codelineno-28-18" href="#__codelineno-28-18"></a> new_cell[index] =
<a id="__codelineno-28-19" name="__codelineno-28-19" href="#__codelineno-28-19"></a> old_cell[index] + candidate_cell[index] * input_gate[index];
<a id="__codelineno-28-20" name="__codelineno-28-20" href="#__codelineno-28-20"></a> new_h[index] = tanh(new_cell[index]) * output_gate[index];
<a id="__codelineno-28-21" name="__codelineno-28-21" href="#__codelineno-28-21"></a> }
<a id="__codelineno-28-22" name="__codelineno-28-22" href="#__codelineno-28-22"></a>}
</code></pre></div>
<p>这里最有趣的是，我们能够为门矩阵中的每个单独组件完全并行地计算所有这些
逐点运算。如果您想象必须使用一个巨大的
 <code>for</code>
 循环来连续处理
 百万个元素，您就会明白为什么这会快得多。</p>
<h3 id="_7">使用访问器 <a href="#using-accessors" title="永久链接到此标题">¶</a></h3>
<p>您可以在 CUDA 内核中看到，我们直接处理具有正确
类型的指针。事实上，直接使用 cuda
内核中的高级类型不可知tensor将非常低效。</p>
<p>然而，这是以易用性和可读性为代价的，特别是对于高维数据。在我们的示例中，我们知道连续
 <code>gates</code>
 tensor具有 3 个维度：</p>
<ol>
<li>批次，大小为
 <code>batch_size</code>
 和步幅
 <code>3*state_size</code>
2.行、大小为
 <code>3</code>
 和步长为
 <code>state_size</code>
3。索引、
 <code>state_size</code> 的大小和 
 <code>1</code> 的步幅</li>
</ol>
<p>那么我们如何在内核内部访问
 <code>gates[n][row][column]</code>
 元素呢？
事实证明，您需要使用一些简单的
arithmetic 来访问元素。</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-29-1" name="__codelineno-29-1" href="#__codelineno-29-1"></a>gates.data&lt;scalar_t&gt;()[n*3*state_size + row*state_size + column]
</code></pre></div>
<p>除了冗长之外，该表达式还需要明确
已知步幅，从而在其参数中传递给内核函数。您可以看到，
如果内核函数接受不同大小的多个tensor，
您最终会得到一个非常长的参数列表。</p>
<p>对我们来说幸运的是，ATen 提供了通过一次动态检查来创建的访问器，
动态检查tensor的类型和维数。
然后访问器公开一个 API，用于有效地访问tensor元素
而无需转换为单个指针:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-30-1" name="__codelineno-30-1" href="#__codelineno-30-1"></a>torch::Tensor foo = torch::rand({12, 12});
<a id="__codelineno-30-2" name="__codelineno-30-2" href="#__codelineno-30-2"></a>
<a id="__codelineno-30-3" name="__codelineno-30-3" href="#__codelineno-30-3"></a>// assert foo is 2-dimensional and holds floats.
<a id="__codelineno-30-4" name="__codelineno-30-4" href="#__codelineno-30-4"></a>auto foo_a = foo.accessor&lt;float,2&gt;();
<a id="__codelineno-30-5" name="__codelineno-30-5" href="#__codelineno-30-5"></a>float trace = 0;
<a id="__codelineno-30-6" name="__codelineno-30-6" href="#__codelineno-30-6"></a>
<a id="__codelineno-30-7" name="__codelineno-30-7" href="#__codelineno-30-7"></a>for(int i = 0; i &lt; foo_a.size(0); i++) {
<a id="__codelineno-30-8" name="__codelineno-30-8" href="#__codelineno-30-8"></a> // use the accessor foo_a to get tensor data.
<a id="__codelineno-30-9" name="__codelineno-30-9" href="#__codelineno-30-9"></a> trace += foo_a[i][i];
<a id="__codelineno-30-10" name="__codelineno-30-10" href="#__codelineno-30-10"></a>}
</code></pre></div>
<p>访问器对象具有相对较高级别的接口，具有
 <code>.size()</code>
 和
 <code>.stride()</code>
 方法和多维索引。 
 <code>.accessor&lt;&gt;</code>
 接口旨在有效地访问 cpu tensor上的数据。 cuda tensor的等效项是 <code>packed_accessor64&lt;&gt;</code>
 和 
 <code>packed_accessor32&lt;&gt;</code>
 ，它们生成具有 64 位或 32 位整数索引的打包访问器。 </p>
<p>与访问器的根本区别在于，打包访问器在其结构内部复制大小
和跨步数据，而不是指向它。它允许我们
将其传递给 CUDA 内核函数并在其中使用其接口。</p>
<p>我们可以设计一个采用打包访问器而不是指针的函数。</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-31-1" name="__codelineno-31-1" href="#__codelineno-31-1"></a>__global__ void lltm_cuda_forward_kernel(
<a id="__codelineno-31-2" name="__codelineno-31-2" href="#__codelineno-31-2"></a> const torch::PackedTensorAccessor32&lt;scalar_t,3,torch::RestrictPtrTraits&gt; gates,
<a id="__codelineno-31-3" name="__codelineno-31-3" href="#__codelineno-31-3"></a> const torch::PackedTensorAccessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt; old_cell,
<a id="__codelineno-31-4" name="__codelineno-31-4" href="#__codelineno-31-4"></a> torch::PackedTensorAccessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt; new_h,
<a id="__codelineno-31-5" name="__codelineno-31-5" href="#__codelineno-31-5"></a> torch::PackedTensorAccessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt; new_cell,
<a id="__codelineno-31-6" name="__codelineno-31-6" href="#__codelineno-31-6"></a> torch::PackedTensorAccessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt; input_gate,
<a id="__codelineno-31-7" name="__codelineno-31-7" href="#__codelineno-31-7"></a> torch::PackedTensorAccessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt; output_gate,
<a id="__codelineno-31-8" name="__codelineno-31-8" href="#__codelineno-31-8"></a> torch::PackedTensorAccessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt; candidate_cell)
</code></pre></div>
<p>让’s 分解这里使用的模板。前两个参数
 <code>scalar_t</code>
 和
 <code>2</code>
 与常规访问器相同。参数
 <code>torch::RestrictPtrTraits</code>
 指示必须
 使用
 <code>__restrict__</code>
 关键字。另请注意，我们’使用了
 <code>PackedAccessor32</code>
 变体，它将大小和步幅存储在</p>
<p><code>int32_t</code>
 中。这很重要，因为使用 64 位
变体 (
 <code>PackedAccessor64</code>
 ) 会使内核变慢。</p>
<p>函数声明变为</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-32-1" name="__codelineno-32-1" href="#__codelineno-32-1"></a>template &lt;typename scalar_t&gt;
<a id="__codelineno-32-2" name="__codelineno-32-2" href="#__codelineno-32-2"></a>__global__ void lltm_cuda_forward_kernel(
<a id="__codelineno-32-3" name="__codelineno-32-3" href="#__codelineno-32-3"></a> const torch::PackedTensorAccessor32&lt;scalar_t,3,torch::RestrictPtrTraits&gt; gates,
<a id="__codelineno-32-4" name="__codelineno-32-4" href="#__codelineno-32-4"></a> const torch::PackedTensorAccessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt; old_cell,
<a id="__codelineno-32-5" name="__codelineno-32-5" href="#__codelineno-32-5"></a> torch::PackedTensorAccessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt; new_h,
<a id="__codelineno-32-6" name="__codelineno-32-6" href="#__codelineno-32-6"></a> torch::PackedTensorAccessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt; new_cell,
<a id="__codelineno-32-7" name="__codelineno-32-7" href="#__codelineno-32-7"></a> torch::PackedTensorAccessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt; input_gate,
<a id="__codelineno-32-8" name="__codelineno-32-8" href="#__codelineno-32-8"></a> torch::PackedTensorAccessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt; output_gate,
<a id="__codelineno-32-9" name="__codelineno-32-9" href="#__codelineno-32-9"></a> torch::PackedTensorAccessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt; candidate_cell) {
<a id="__codelineno-32-10" name="__codelineno-32-10" href="#__codelineno-32-10"></a> //batch index
<a id="__codelineno-32-11" name="__codelineno-32-11" href="#__codelineno-32-11"></a> const int n = blockIdx.y;
<a id="__codelineno-32-12" name="__codelineno-32-12" href="#__codelineno-32-12"></a> // column index
<a id="__codelineno-32-13" name="__codelineno-32-13" href="#__codelineno-32-13"></a> const int c = blockIdx.x * blockDim.x + threadIdx.x;
<a id="__codelineno-32-14" name="__codelineno-32-14" href="#__codelineno-32-14"></a> if (c &lt; gates.size(2)){
<a id="__codelineno-32-15" name="__codelineno-32-15" href="#__codelineno-32-15"></a> input_gate[n][c] = sigmoid(gates[n][0][c]);
<a id="__codelineno-32-16" name="__codelineno-32-16" href="#__codelineno-32-16"></a> output_gate[n][c] = sigmoid(gates[n][1][c]);
<a id="__codelineno-32-17" name="__codelineno-32-17" href="#__codelineno-32-17"></a> candidate_cell[n][c] = elu(gates[n][2][c]);
<a id="__codelineno-32-18" name="__codelineno-32-18" href="#__codelineno-32-18"></a> new_cell[n][c] =
<a id="__codelineno-32-19" name="__codelineno-32-19" href="#__codelineno-32-19"></a> old_cell[n][c] + candidate_cell[n][c] * input_gate[n][c];
<a id="__codelineno-32-20" name="__codelineno-32-20" href="#__codelineno-32-20"></a> new_h[n][c] = tanh(new_cell[n][c]) * output_gate[n][c];
<a id="__codelineno-32-21" name="__codelineno-32-21" href="#__codelineno-32-21"></a> }
<a id="__codelineno-32-22" name="__codelineno-32-22" href="#__codelineno-32-22"></a>}
</code></pre></div>
<p>实现更具可读性！然后通过使用主机函数中的
 <code>.packed_accessor32&lt;&gt;</code>
 方法创建
打包访问器来调用该函数。</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-33-1" name="__codelineno-33-1" href="#__codelineno-33-1"></a>std::vector&lt;torch::Tensor&gt; lltm_cuda_forward(
<a id="__codelineno-33-2" name="__codelineno-33-2" href="#__codelineno-33-2"></a> torch::Tensor input,
<a id="__codelineno-33-3" name="__codelineno-33-3" href="#__codelineno-33-3"></a> torch::Tensor weights,
<a id="__codelineno-33-4" name="__codelineno-33-4" href="#__codelineno-33-4"></a> torch::Tensor bias,
<a id="__codelineno-33-5" name="__codelineno-33-5" href="#__codelineno-33-5"></a> torch::Tensor old_h,
<a id="__codelineno-33-6" name="__codelineno-33-6" href="#__codelineno-33-6"></a> torch::Tensor old_cell) {
<a id="__codelineno-33-7" name="__codelineno-33-7" href="#__codelineno-33-7"></a> auto X = torch::cat({old_h, input}, /*dim=*/1);
<a id="__codelineno-33-8" name="__codelineno-33-8" href="#__codelineno-33-8"></a> auto gate_weights = torch::addmm(bias, X, weights.transpose(0, 1));
<a id="__codelineno-33-9" name="__codelineno-33-9" href="#__codelineno-33-9"></a>
<a id="__codelineno-33-10" name="__codelineno-33-10" href="#__codelineno-33-10"></a> const auto batch_size = old_cell.size(0);
<a id="__codelineno-33-11" name="__codelineno-33-11" href="#__codelineno-33-11"></a> const auto state_size = old_cell.size(1);
<a id="__codelineno-33-12" name="__codelineno-33-12" href="#__codelineno-33-12"></a>
<a id="__codelineno-33-13" name="__codelineno-33-13" href="#__codelineno-33-13"></a> auto gates = gate_weights.reshape({batch_size, 3, state_size});
<a id="__codelineno-33-14" name="__codelineno-33-14" href="#__codelineno-33-14"></a> auto new_h = torch::zeros_like(old_cell);
<a id="__codelineno-33-15" name="__codelineno-33-15" href="#__codelineno-33-15"></a> auto new_cell = torch::zeros_like(old_cell);
<a id="__codelineno-33-16" name="__codelineno-33-16" href="#__codelineno-33-16"></a> auto input_gate = torch::zeros_like(old_cell);
<a id="__codelineno-33-17" name="__codelineno-33-17" href="#__codelineno-33-17"></a> auto output_gate = torch::zeros_like(old_cell);
<a id="__codelineno-33-18" name="__codelineno-33-18" href="#__codelineno-33-18"></a> auto candidate_cell = torch::zeros_like(old_cell);
<a id="__codelineno-33-19" name="__codelineno-33-19" href="#__codelineno-33-19"></a>
<a id="__codelineno-33-20" name="__codelineno-33-20" href="#__codelineno-33-20"></a> const int threads = 1024;
<a id="__codelineno-33-21" name="__codelineno-33-21" href="#__codelineno-33-21"></a> const dim3 blocks((state_size + threads - 1) / threads, batch_size);
<a id="__codelineno-33-22" name="__codelineno-33-22" href="#__codelineno-33-22"></a>
<a id="__codelineno-33-23" name="__codelineno-33-23" href="#__codelineno-33-23"></a> AT_DISPATCH_FLOATING_TYPES(gates.type(), &quot;lltm_forward_cuda&quot;, ([&amp;] {
<a id="__codelineno-33-24" name="__codelineno-33-24" href="#__codelineno-33-24"></a> lltm_cuda_forward_kernel&lt;scalar_t&gt;&lt;&lt;&lt;blocks, threads&gt;&gt;&gt;(
<a id="__codelineno-33-25" name="__codelineno-33-25" href="#__codelineno-33-25"></a> gates.packed_accessor32&lt;scalar_t,3,torch::RestrictPtrTraits&gt;(),
<a id="__codelineno-33-26" name="__codelineno-33-26" href="#__codelineno-33-26"></a> old_cell.packed_accessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt;(),
<a id="__codelineno-33-27" name="__codelineno-33-27" href="#__codelineno-33-27"></a> new_h.packed_accessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt;(),
<a id="__codelineno-33-28" name="__codelineno-33-28" href="#__codelineno-33-28"></a> new_cell.packed_accessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt;(),
<a id="__codelineno-33-29" name="__codelineno-33-29" href="#__codelineno-33-29"></a> input_gate.packed_accessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt;(),
<a id="__codelineno-33-30" name="__codelineno-33-30" href="#__codelineno-33-30"></a> output_gate.packed_accessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt;(),
<a id="__codelineno-33-31" name="__codelineno-33-31" href="#__codelineno-33-31"></a> candidate_cell.packed_accessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt;());
<a id="__codelineno-33-32" name="__codelineno-33-32" href="#__codelineno-33-32"></a> }));
<a id="__codelineno-33-33" name="__codelineno-33-33" href="#__codelineno-33-33"></a>
<a id="__codelineno-33-34" name="__codelineno-33-34" href="#__codelineno-33-34"></a> return {new_h, new_cell, input_gate, output_gate, candidate_cell, X, gates};
<a id="__codelineno-33-35" name="__codelineno-33-35" href="#__codelineno-33-35"></a>}
</code></pre></div>
<p>向后传递遵循大致相同的模式，我不会’ 进一步详细说明
：</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-34-1" name="__codelineno-34-1" href="#__codelineno-34-1"></a>template &lt;typename scalar_t&gt;
<a id="__codelineno-34-2" name="__codelineno-34-2" href="#__codelineno-34-2"></a>__global__ void lltm_cuda_backward_kernel(
<a id="__codelineno-34-3" name="__codelineno-34-3" href="#__codelineno-34-3"></a> torch::PackedTensorAccessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt; d_old_cell,
<a id="__codelineno-34-4" name="__codelineno-34-4" href="#__codelineno-34-4"></a> torch::PackedTensorAccessor32&lt;scalar_t,3,torch::RestrictPtrTraits&gt; d_gates,
<a id="__codelineno-34-5" name="__codelineno-34-5" href="#__codelineno-34-5"></a> const torch::PackedTensorAccessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt; grad_h,
<a id="__codelineno-34-6" name="__codelineno-34-6" href="#__codelineno-34-6"></a> const torch::PackedTensorAccessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt; grad_cell,
<a id="__codelineno-34-7" name="__codelineno-34-7" href="#__codelineno-34-7"></a> const torch::PackedTensorAccessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt; new_cell,
<a id="__codelineno-34-8" name="__codelineno-34-8" href="#__codelineno-34-8"></a> const torch::PackedTensorAccessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt; input_gate,
<a id="__codelineno-34-9" name="__codelineno-34-9" href="#__codelineno-34-9"></a> const torch::PackedTensorAccessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt; output_gate,
<a id="__codelineno-34-10" name="__codelineno-34-10" href="#__codelineno-34-10"></a> const torch::PackedTensorAccessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt; candidate_cell,
<a id="__codelineno-34-11" name="__codelineno-34-11" href="#__codelineno-34-11"></a> const torch::PackedTensorAccessor32&lt;scalar_t,3,torch::RestrictPtrTraits&gt; gate_weights) {
<a id="__codelineno-34-12" name="__codelineno-34-12" href="#__codelineno-34-12"></a> //batch index
<a id="__codelineno-34-13" name="__codelineno-34-13" href="#__codelineno-34-13"></a> const int n = blockIdx.y;
<a id="__codelineno-34-14" name="__codelineno-34-14" href="#__codelineno-34-14"></a> // column index
<a id="__codelineno-34-15" name="__codelineno-34-15" href="#__codelineno-34-15"></a> const int c = blockIdx.x * blockDim.x + threadIdx.x;
<a id="__codelineno-34-16" name="__codelineno-34-16" href="#__codelineno-34-16"></a> if (c &lt; d_gates.size(2)){
<a id="__codelineno-34-17" name="__codelineno-34-17" href="#__codelineno-34-17"></a> const auto d_output_gate = tanh(new_cell[n][c]) * grad_h[n][c];
<a id="__codelineno-34-18" name="__codelineno-34-18" href="#__codelineno-34-18"></a> const auto d_tanh_new_cell = output_gate[n][c] * grad_h[n][c];
<a id="__codelineno-34-19" name="__codelineno-34-19" href="#__codelineno-34-19"></a> const auto d_new_cell =
<a id="__codelineno-34-20" name="__codelineno-34-20" href="#__codelineno-34-20"></a> d_tanh(new_cell[n][c]) * d_tanh_new_cell + grad_cell[n][c];
<a id="__codelineno-34-21" name="__codelineno-34-21" href="#__codelineno-34-21"></a>
<a id="__codelineno-34-22" name="__codelineno-34-22" href="#__codelineno-34-22"></a>
<a id="__codelineno-34-23" name="__codelineno-34-23" href="#__codelineno-34-23"></a> d_old_cell[n][c] = d_new_cell;
<a id="__codelineno-34-24" name="__codelineno-34-24" href="#__codelineno-34-24"></a> const auto d_candidate_cell = input_gate[n][c] * d_new_cell;
<a id="__codelineno-34-25" name="__codelineno-34-25" href="#__codelineno-34-25"></a> const auto d_input_gate = candidate_cell[n][c] * d_new_cell;
<a id="__codelineno-34-26" name="__codelineno-34-26" href="#__codelineno-34-26"></a>
<a id="__codelineno-34-27" name="__codelineno-34-27" href="#__codelineno-34-27"></a> d_gates[n][0][c] =
<a id="__codelineno-34-28" name="__codelineno-34-28" href="#__codelineno-34-28"></a> d_input_gate * d_sigmoid(gate_weights[n][0][c]);
<a id="__codelineno-34-29" name="__codelineno-34-29" href="#__codelineno-34-29"></a> d_gates[n][1][c] =
<a id="__codelineno-34-30" name="__codelineno-34-30" href="#__codelineno-34-30"></a> d_output_gate * d_sigmoid(gate_weights[n][1][c]);
<a id="__codelineno-34-31" name="__codelineno-34-31" href="#__codelineno-34-31"></a> d_gates[n][2][c] =
<a id="__codelineno-34-32" name="__codelineno-34-32" href="#__codelineno-34-32"></a> d_candidate_cell * d_elu(gate_weights[n][2][c]);
<a id="__codelineno-34-33" name="__codelineno-34-33" href="#__codelineno-34-33"></a> }
<a id="__codelineno-34-34" name="__codelineno-34-34" href="#__codelineno-34-34"></a>}
<a id="__codelineno-34-35" name="__codelineno-34-35" href="#__codelineno-34-35"></a>
<a id="__codelineno-34-36" name="__codelineno-34-36" href="#__codelineno-34-36"></a>std::vector&lt;torch::Tensor&gt; lltm_cuda_backward(
<a id="__codelineno-34-37" name="__codelineno-34-37" href="#__codelineno-34-37"></a> torch::Tensor grad_h,
<a id="__codelineno-34-38" name="__codelineno-34-38" href="#__codelineno-34-38"></a> torch::Tensor grad_cell,
<a id="__codelineno-34-39" name="__codelineno-34-39" href="#__codelineno-34-39"></a> torch::Tensor new_cell,
<a id="__codelineno-34-40" name="__codelineno-34-40" href="#__codelineno-34-40"></a> torch::Tensor input_gate,
<a id="__codelineno-34-41" name="__codelineno-34-41" href="#__codelineno-34-41"></a> torch::Tensor output_gate,
<a id="__codelineno-34-42" name="__codelineno-34-42" href="#__codelineno-34-42"></a> torch::Tensor candidate_cell,
<a id="__codelineno-34-43" name="__codelineno-34-43" href="#__codelineno-34-43"></a> torch::Tensor X,
<a id="__codelineno-34-44" name="__codelineno-34-44" href="#__codelineno-34-44"></a> torch::Tensor gates,
<a id="__codelineno-34-45" name="__codelineno-34-45" href="#__codelineno-34-45"></a> torch::Tensor weights) {
<a id="__codelineno-34-46" name="__codelineno-34-46" href="#__codelineno-34-46"></a> auto d_old_cell = torch::zeros_like(new_cell);
<a id="__codelineno-34-47" name="__codelineno-34-47" href="#__codelineno-34-47"></a> auto d_gates = torch::zeros_like(gates);
<a id="__codelineno-34-48" name="__codelineno-34-48" href="#__codelineno-34-48"></a>
<a id="__codelineno-34-49" name="__codelineno-34-49" href="#__codelineno-34-49"></a> const auto batch_size = new_cell.size(0);
<a id="__codelineno-34-50" name="__codelineno-34-50" href="#__codelineno-34-50"></a> const auto state_size = new_cell.size(1);
<a id="__codelineno-34-51" name="__codelineno-34-51" href="#__codelineno-34-51"></a>
<a id="__codelineno-34-52" name="__codelineno-34-52" href="#__codelineno-34-52"></a> const int threads = 1024;
<a id="__codelineno-34-53" name="__codelineno-34-53" href="#__codelineno-34-53"></a> const dim3 blocks((state_size + threads - 1) / threads, batch_size);
<a id="__codelineno-34-54" name="__codelineno-34-54" href="#__codelineno-34-54"></a>
<a id="__codelineno-34-55" name="__codelineno-34-55" href="#__codelineno-34-55"></a> AT_DISPATCH_FLOATING_TYPES(X.type(), &quot;lltm_backward_cuda&quot;, ([&amp;] {
<a id="__codelineno-34-56" name="__codelineno-34-56" href="#__codelineno-34-56"></a> lltm_cuda_backward_kernel&lt;scalar_t&gt;&lt;&lt;&lt;blocks, threads&gt;&gt;&gt;(
<a id="__codelineno-34-57" name="__codelineno-34-57" href="#__codelineno-34-57"></a> d_old_cell.packed_accessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt;(),
<a id="__codelineno-34-58" name="__codelineno-34-58" href="#__codelineno-34-58"></a> d_gates.packed_accessor32&lt;scalar_t,3,torch::RestrictPtrTraits&gt;(),
<a id="__codelineno-34-59" name="__codelineno-34-59" href="#__codelineno-34-59"></a> grad_h.packed_accessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt;(),
<a id="__codelineno-34-60" name="__codelineno-34-60" href="#__codelineno-34-60"></a> grad_cell.packed_accessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt;(),
<a id="__codelineno-34-61" name="__codelineno-34-61" href="#__codelineno-34-61"></a> new_cell.packed_accessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt;(),
<a id="__codelineno-34-62" name="__codelineno-34-62" href="#__codelineno-34-62"></a> input_gate.packed_accessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt;(),
<a id="__codelineno-34-63" name="__codelineno-34-63" href="#__codelineno-34-63"></a> output_gate.packed_accessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt;(),
<a id="__codelineno-34-64" name="__codelineno-34-64" href="#__codelineno-34-64"></a> candidate_cell.packed_accessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt;(),
<a id="__codelineno-34-65" name="__codelineno-34-65" href="#__codelineno-34-65"></a> gates.packed_accessor32&lt;scalar_t,3,torch::RestrictPtrTraits&gt;());
<a id="__codelineno-34-66" name="__codelineno-34-66" href="#__codelineno-34-66"></a> }));
<a id="__codelineno-34-67" name="__codelineno-34-67" href="#__codelineno-34-67"></a>
<a id="__codelineno-34-68" name="__codelineno-34-68" href="#__codelineno-34-68"></a> auto d_gate_weights = d_gates.reshape({batch_size, 3*state_size});
<a id="__codelineno-34-69" name="__codelineno-34-69" href="#__codelineno-34-69"></a> auto d_weights = d_gate_weights.t().mm(X);
<a id="__codelineno-34-70" name="__codelineno-34-70" href="#__codelineno-34-70"></a> auto d_bias = d_gate_weights.sum(/*dim=*/0, /*keepdim=*/true);
<a id="__codelineno-34-71" name="__codelineno-34-71" href="#__codelineno-34-71"></a>
<a id="__codelineno-34-72" name="__codelineno-34-72" href="#__codelineno-34-72"></a> auto d_X = d_gate_weights.mm(weights);
<a id="__codelineno-34-73" name="__codelineno-34-73" href="#__codelineno-34-73"></a> auto d_old_h = d_X.slice(/*dim=*/1, 0, state_size);
<a id="__codelineno-34-74" name="__codelineno-34-74" href="#__codelineno-34-74"></a> auto d_input = d_X.slice(/*dim=*/1, state_size);
<a id="__codelineno-34-75" name="__codelineno-34-75" href="#__codelineno-34-75"></a>
<a id="__codelineno-34-76" name="__codelineno-34-76" href="#__codelineno-34-76"></a> return {d_old_h, d_input, d_weights, d_bias, d_old_cell, d_gates};
<a id="__codelineno-34-77" name="__codelineno-34-77" href="#__codelineno-34-77"></a>}
</code></pre></div>
<h3 id="ccuda-pytorch">将 C++/CUDA 操作与 PyTorch 集成 <a href="#integrating-a-c-cuda-operation-with-pytorch" title="永久链接到此标题">¶</a></h3>
<p>我们支持 CUDA 的操作与 PyTorch 的集成同样非常简单。
如果您想编写
 <code>setup.py</code>
 脚本，它可能如下所示：</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-35-1" name="__codelineno-35-1" href="#__codelineno-35-1"></a>from setuptools import setup
<a id="__codelineno-35-2" name="__codelineno-35-2" href="#__codelineno-35-2"></a>from torch.utils.cpp_extension import BuildExtension, CUDAExtension
<a id="__codelineno-35-3" name="__codelineno-35-3" href="#__codelineno-35-3"></a>
<a id="__codelineno-35-4" name="__codelineno-35-4" href="#__codelineno-35-4"></a>setup(
<a id="__codelineno-35-5" name="__codelineno-35-5" href="#__codelineno-35-5"></a>    name=&#39;lltm&#39;,
<a id="__codelineno-35-6" name="__codelineno-35-6" href="#__codelineno-35-6"></a>    ext_modules=[
<a id="__codelineno-35-7" name="__codelineno-35-7" href="#__codelineno-35-7"></a>        CUDAExtension(&#39;lltm_cuda&#39;, [
<a id="__codelineno-35-8" name="__codelineno-35-8" href="#__codelineno-35-8"></a>            &#39;lltm_cuda.cpp&#39;,
<a id="__codelineno-35-9" name="__codelineno-35-9" href="#__codelineno-35-9"></a>            &#39;lltm_cuda_kernel.cu&#39;,
<a id="__codelineno-35-10" name="__codelineno-35-10" href="#__codelineno-35-10"></a>        ])
<a id="__codelineno-35-11" name="__codelineno-35-11" href="#__codelineno-35-11"></a>    ],
<a id="__codelineno-35-12" name="__codelineno-35-12" href="#__codelineno-35-12"></a>    cmdclass={
<a id="__codelineno-35-13" name="__codelineno-35-13" href="#__codelineno-35-13"></a>        &#39;build_ext&#39;: BuildExtension
<a id="__codelineno-35-14" name="__codelineno-35-14" href="#__codelineno-35-14"></a>    })
</code></pre></div>
<p>我们现在使用
 <code>CUDAExtension()</code>
 而不是
 <code>CppExtension()</code>
 。我们只需
指定
 <code>.cu</code>
 文件以及
 <code>.cpp</code>
 文件–，库会
处理这给您带来的所有麻烦。 JIT 机制甚至
更简单:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-36-1" name="__codelineno-36-1" href="#__codelineno-36-1"></a>from torch.utils.cpp_extension import load
<a id="__codelineno-36-2" name="__codelineno-36-2" href="#__codelineno-36-2"></a>
<a id="__codelineno-36-3" name="__codelineno-36-3" href="#__codelineno-36-3"></a>lltm = load(name=&#39;lltm&#39;, sources=[&#39;lltm_cuda.cpp&#39;, &#39;lltm_cuda_kernel.cu&#39;])
</code></pre></div>
<h4 id="_8">性能比较 <a href="#id4" title="此标题的永久链接">¶</a></h4>
<p>我们希望将代码的逐点操作与 CUDA 进行并行化和融合
能够提高 LLTM 的性能。让’s 看看这是否成立。
我们可以运行我之前列出的代码来运行基准测试。我们之前最快的
版本是基于 CUDA 的 C++ 代码：</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-37-1" name="__codelineno-37-1" href="#__codelineno-37-1"></a>Forward: 149.802 us | Backward 393.458 us
</code></pre></div>
<p>现在使用我们的自定义 CUDA 内核：</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-38-1" name="__codelineno-38-1" href="#__codelineno-38-1"></a>Forward: 129.431 us | Backward 304.641 us
</code></pre></div>
<p>更多性能提升！</p>
<h2 id="_9">结论 <a href="#conclusion" title="永久链接到此标题">¶</a></h2>
<p>您现在应该对 PyTorch’s C++ 扩展
机制以及使用它们的动机有一个很好的概述。您可以在<a href="https://github.com/pytorch/extension-cpp">此处</a>
 找到本说明中显示的代码
示例。如果您有疑问，请使用
 <a href="https://discuss.pytorch.org">论坛</a> 
 。另请务必查看我们的
 <a href="https://pytorch.org/cppdocs/notes/faq.html">常见问题解答</a>
，以防遇到任何问题。</p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href="../../intermediate/custom_function_conv_bn_tutorial/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Fusing Convolution and Batch Norm using Custom Function">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                Fusing Convolution and Batch Norm using Custom Function
              </div>
            </div>
          </a>
        
        
          
          <a href="../torch_script_custom_ops/" class="md-footer__link md-footer__link--next" aria-label="Next: Extending TorchScript with Custom C++ Operators">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                Extending TorchScript with Custom C++ Operators
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../../../..", "features": ["content.code.copy", "content.action.edit", "content.action.view", "navigation.footer"], "search": "../../../../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../../../assets/javascripts/bundle.f1b6f286.min.js"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>