from .module import Module
from .. import functional as F


class Dropout(Module):
    r"""Dropout在训练期间，按照伯努利概率分布，以概率p随机地将输入张量中的部分元素
	置为0，在每次调用中，被置为0的元素是随机的。

	Dropout已被证明是正则化的一个行之有效的技术，并且在防止神经元之间互适应问题上
	也卓有成效。（神经元互适应问题详见论文“Improving neural networks by preventing 
	co-adaptation of feature detectors”）

	并且，Dropout的输出均与*1/(1-p)*的比例系数进行了相乘，保证了求值时函数的归一化。
    
	参数：
		p：元素被置为0的概率，默认值：0.5
		inplace：如果为“True”，置0操作将直接发生在传入的元素上。默认值：“false”
    
	数据大小：
		- Input：'any'。输入数据可以是任何大小
		- Output：'Same'。输出数据大小与输入相同
    
	示例：

		>>> m = nn.Dropout(p=0.2)
        >>> input = autograd.Variable(torch.randn(20, 16))
        >>> output = m(input)

    .. _Improving neural networks by preventing co-adaptation of feature
        detectors: https://arxiv.org/abs/1207.0580
    """

    def __init__(self, p=0.5, inplace=False):
        super(Dropout, self).__init__()
        if p < 0 or p > 1:
            raise ValueError("dropout probability has to be between 0 and 1, "
                             "but got {}".format(p))
        self.p = p
        self.inplace = inplace

    def forward(self, input):
        return F.dropout(input, self.p, self.training, self.inplace)

    def __repr__(self):
        inplace_str = ', inplace' if self.inplace else ''
        return self.__class__.__name__ + '(' \
            + 'p=' + str(self.p) \
            + inplace_str + ')'


class Dropout2d(Module):
    r"""Dropout2d将输入张量的所有通道随机地置为0。被置为0的通道在每次调用时
	是随机地。
	
	通常输入数据来自Conv2d模块。

	在论文“Efficient Object Localization Using Convolutional Networks`_ ”中有如下
	描述：如果特征映射中的邻接像素是强相关的（在早期的卷积层中很常见），那么独立同分布
	的dropout将不会正则化激活函数，相反其会导致有效的学习率的下降。

	在这样的情况下，应该使用函数函数'nn.Dropout2d'，它能够促进特征映射之间的独立性。
    
	参数：
		p (float,optional)：元素被置0的概率
		inplace（bool，optional）：如果被设为’True’，置0操作将直接作用在输入元素上
   
	数据大小：
		- Input：math:(N, C, H, W)
		- Output：math:(N, C, H, W) （与输入相同）
    
	示例：

		>>> m = nn.Dropout2d(p=0.2)
        >>> input = autograd.Variable(torch.randn(20, 16, 32, 32))
        >>> output = m(input)

    .. _Efficient Object Localization Using Convolutional Networks:
       http://arxiv.org/abs/1411.4280
    """

    def __init__(self, p=0.5, inplace=False):
        super(Dropout2d, self).__init__()
        if p < 0 or p > 1:
            raise ValueError("dropout probability has to be between 0 and 1, "
                             "but got {}".format(p))
        self.p = p
        self.inplace = inplace

    def forward(self, input):
        return F.dropout2d(input, self.p, self.training, self.inplace)

    def __repr__(self):
        inplace_str = ', inplace' if self.inplace else ''
        return self.__class__.__name__ + '(' \
            + 'p=' + str(self.p) \
            + inplace_str + ')'


class Dropout3d(Module):
    r"""Dropout2d将输入张量的所有通道随机地置为0。被置为0的通道在每次调用时
	是随机地。
	
	通常输入数据来自Conv3d模块。

	在论文“Efficient Object Localization Using Convolutional Networks`_ ”中有如下
	描述：如果特征映射中的邻接像素是强相关的（在早期的卷积层中很常见），那么独立同分布
	的dropout将不会正则化激活函数，相反其会导致有效的学习率的下降。

	在这样的情况下，应该使用函数函数'nn.Dropout3d'，它能够促进特征映射之间的独立性。
    
	参数：
		p (float,optional)：元素被置0的概率
		inplace（bool，optional）：如果被设为’True’，置0操作将直接作用在输入元素上
    
	数据大小：
		- Input：math:(N, C, H, W)
		- Output：math:(N, C, H, W) （与输入相同）
    
	示例：

		>>> m = nn.Dropout3d(p=0.2)
        >>> input = autograd.Variable(torch.randn(20, 16, 4, 32, 32))
        >>> output = m(input)
    
    .. _Efficient Object Localization Using Convolutional Networks:
       http://arxiv.org/abs/1411.4280
    """

    def __init__(self, p=0.5, inplace=False):
        super(Dropout3d, self).__init__()
        if p < 0 or p > 1:
            raise ValueError("dropout probability has to be between 0 and 1, "
                             "but got {}".format(p))
        self.p = p
        self.inplace = inplace

    def forward(self, input):
        return F.dropout3d(input, self.p, self.training, self.inplace)

    def __repr__(self):
        inplace_str = ', inplace' if self.inplace else ''
        return self.__class__.__name__ + '(' \
            + 'p=' + str(self.p) \
            + inplace_str + ')'


class AlphaDropout(Module):
    r"""在输入上应用Alpha Dropout。

	Alpha Dropout是一种维持自正交性质的Dropout。对于一个均值为0和标准差为1的输入
	来说，Alpha Dropout能保持原始数据的均值和标准差。Alpha Dropout和SELU激活函数
	携手同行，后者也保证了输出拥有与输入相同的均值和标准差。
    
	Alpha Dropout在训练期间，按照伯努利概率分布，以概率p随机地将输入张量中的部分元素
	置进行掩盖，在每次调用中，被掩盖的元素是随机的，并且对输出会进行缩放、变换等操作
	以保持均值为0、标准差为1.

	在求值期间，模块简单的计算一个归一化的函数。

	更多信息请参考论文：Self-Normalizing Neural Networks

	参数：
		p（float）：元素被掩盖的概率，默认值：0.5
    
	数据大小：
		- Input：'any'。输入数据可以是任何大小
		- Output：'Same'。输出数据大小与输入相同

	示例：

		>>> m = nn.AlphaDropout(p=0.2)
        >>> input = autograd.Variable(torch.randn(20, 16))
        >>> output = m(input)

    .. _Self-Normalizing Neural Networks: https://arxiv.org/abs/1706.02515
    """

    def __init__(self, p=0.5):
        super(AlphaDropout, self).__init__()
        if p < 0 or p > 1:
            raise ValueError("dropout probability has to be between 0 and 1, "
                             "but got {}".format(p))
        self.p = p

    def forward(self, input):
        return F.alpha_dropout(input, self.p, self.training)

    def __repr__(self):
        return self.__class__.__name__ + '(' \
            + 'p=' + str(self.p) + ')'
